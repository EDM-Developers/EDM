{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Empirical Dynamic Modeling Stata Package Package Description Empirical Dynamic Modeling (EDM) is a way to perform causal analysis on time series data . The edm Stata package implements a series of EDM tools, including the convergent cross-mapping algorithm. Key features of the package: powered by a fast multi-threaded C++ backend , able to process panel data, a.k.a. multispatial EDM , able to handle missing data using new dt algorithms or by dropping points, factor variables can be added to the analysis, multiple distance functions available (Euclidean, Mean Absolute Error, Wasserstein), GPU acceleration available. Installation To install the stable version directly through Stata: ssc install edm, replace To install the latest development version, first install the stable version from SSC then inside Stata run: edm update , development replace The source code for the package is available on Github . R package We are currently creating the fastEDM R package which is a direct port of this Stata package to R. As both packages share the same underlying C++ code, their behaviour will be identical. Other Resources This site serves as the primary source of documentation for the package, though there is also: our Stata Journal paper which explains the package and the overall causal framework, and Jinjing's QMNET seminar on the package, the recording is on YouTube and the slides are here . Authors Jinjing Li (author), Michael Zyphur (author), Patrick Laub (author, maintainer), Edoardo Tescari (contributor), Simon Mutch (contributor), George Sugihara (originator) Citation Jinjing Li, Michael J. Zyphur, George Sugihara, Patrick J. Laub (2021), Beyond Linearity, Stability, and Equilibrium: The edm Package for Empirical Dynamic Modeling and Convergent Cross Mapping in Stata , Stata Journal, 21(1), pp. 220-258 @article { edm-stata , title = {Beyond linearity, stability, and equilibrium: The edm package for empirical dynamic modeling and convergent cross-mapping in {S}tata} , author = {Li, Jinjing and Zyphur, Michael J and Sugihara, George and Laub, Patrick J} , journal = {The Stata Journal} , volume = {21} , number = {1} , pages = {220--258} , year = {2021} , }","title":"Home"},{"location":"#empirical-dynamic-modeling-stata-package","text":"","title":"Empirical Dynamic Modeling Stata Package"},{"location":"#package-description","text":"Empirical Dynamic Modeling (EDM) is a way to perform causal analysis on time series data . The edm Stata package implements a series of EDM tools, including the convergent cross-mapping algorithm. Key features of the package: powered by a fast multi-threaded C++ backend , able to process panel data, a.k.a. multispatial EDM , able to handle missing data using new dt algorithms or by dropping points, factor variables can be added to the analysis, multiple distance functions available (Euclidean, Mean Absolute Error, Wasserstein), GPU acceleration available.","title":"Package Description"},{"location":"#installation","text":"To install the stable version directly through Stata: ssc install edm, replace To install the latest development version, first install the stable version from SSC then inside Stata run: edm update , development replace The source code for the package is available on Github .","title":"Installation"},{"location":"#r-package","text":"We are currently creating the fastEDM R package which is a direct port of this Stata package to R. As both packages share the same underlying C++ code, their behaviour will be identical.","title":"R package"},{"location":"#other-resources","text":"This site serves as the primary source of documentation for the package, though there is also: our Stata Journal paper which explains the package and the overall causal framework, and Jinjing's QMNET seminar on the package, the recording is on YouTube and the slides are here .","title":"Other Resources"},{"location":"#authors","text":"Jinjing Li (author), Michael Zyphur (author), Patrick Laub (author, maintainer), Edoardo Tescari (contributor), Simon Mutch (contributor), George Sugihara (originator)","title":"Authors"},{"location":"#citation","text":"Jinjing Li, Michael J. Zyphur, George Sugihara, Patrick J. Laub (2021), Beyond Linearity, Stability, and Equilibrium: The edm Package for Empirical Dynamic Modeling and Convergent Cross Mapping in Stata , Stata Journal, 21(1), pp. 220-258 @article { edm-stata , title = {Beyond linearity, stability, and equilibrium: The edm package for empirical dynamic modeling and convergent cross-mapping in {S}tata} , author = {Li, Jinjing and Zyphur, Michael J and Sugihara, George and Laub, Patrick J} , journal = {The Stata Journal} , volume = {21} , number = {1} , pages = {220--258} , year = {2021} , }","title":"Citation"},{"location":"adding-to-manifold/","text":"Adding extra variables to the manifold It can be advantageous to combine data from multiple sources into a single EDM analysis. The extra command will incorporate additional pieces of data into the manifold. As an example, consider the Stata command edm explore x, extra(y) Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with the given \\(E\\) and \\(\\tau\\) is the manifold: However, with the extra(y) option, we use the time-delayed embedding with the extra time series included like: After extra variables are added, the manifold \\(M_{x,y}\\) no longer has \\(E\\) columns. In these cases, we make a distinction between \\(E\\) which selects the number of lags for each time series, and the actual \\(E\\) which is size of each point (i.e. the number of columns). By default just one \\(y\\) observation is added to each point in the manifold. If \\(E\\) lags of \\(y\\) are required, then the command should be altered slightly to edm explore x, extra(y(e)) and then the manifold will be: More than one extra variable can be added, and any combinations of \\(E\\) -varying and non- \\(E\\) -varying extras are permitted. Note If some extras are lagged extra variables (i.e. \\(E\\) -varying extras) and they are specified after some unlagged extras, then the package will reorder them so that all the lagged extras are first.","title":"Adding variables to manifold"},{"location":"adding-to-manifold/#adding-extra-variables-to-the-manifold","text":"It can be advantageous to combine data from multiple sources into a single EDM analysis. The extra command will incorporate additional pieces of data into the manifold. As an example, consider the Stata command edm explore x, extra(y) Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with the given \\(E\\) and \\(\\tau\\) is the manifold: However, with the extra(y) option, we use the time-delayed embedding with the extra time series included like: After extra variables are added, the manifold \\(M_{x,y}\\) no longer has \\(E\\) columns. In these cases, we make a distinction between \\(E\\) which selects the number of lags for each time series, and the actual \\(E\\) which is size of each point (i.e. the number of columns). By default just one \\(y\\) observation is added to each point in the manifold. If \\(E\\) lags of \\(y\\) are required, then the command should be altered slightly to edm explore x, extra(y(e)) and then the manifold will be: More than one extra variable can be added, and any combinations of \\(E\\) -varying and non- \\(E\\) -varying extras are permitted. Note If some extras are lagged extra variables (i.e. \\(E\\) -varying extras) and they are specified after some unlagged extras, then the package will reorder them so that all the lagged extras are first.","title":"Adding extra variables to the manifold"},{"location":"api/","text":"Stata package help file Note This page simply reproduces, for convenience, the output of typing help edm into the Stata console. The command edm implements a series of tools that can be used for empirical dynamic modeling in Stata. The core algorithm is written in C++ (with a Mata backup) to achieve reasonable execution speed. The command keyword is edm , and should be immediately followed by a subcommand such as explore or xmap. A dataset must be declared as time-series or panel data by the tsset or xtset command prior to using the edm command, and time-series operators including l., f., d., and s. can be used (the last for seasonal differencing). Syntax The explore subcommand follows the syntax below and supports one variable for exploration using simplex projection or S-mapping. edm explore variable [if exp], [ e (numlist ascending >= 2 )] [tau(integer 1 )] [theta(numlist ascending)] [k(integer 0 )] [ALGorithm(string)] [REPlicate(integer 0 )] [seed(integer 0 )] [full] [RANDomize] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)] [CROSSfold(integer 0 )] [CI(integer 0 )] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0 )] [dt] [reldt] [DTWeight(real 0 )] [DTSave(name)] [DETails] [reportrawe] [strict] [Predictionhorizon(string)] [dot(integer 1 )] [mata] [gpu] [nthreads(integer 0 )] [savemanifold(name)] [idw(real 0 )] [lowmemory] The second subcommand xmap performs convergent cross-mapping (CCM). The subcommand follows the syntax below and requires two variables to follow immediately after xmap. It shares many of the same options with the explore subcommand although there are some differences given the different purpose of the analysis. edm xmap variables [if exp], [ e (integer 2 )] [tau(integer 1 )] [theta(real 1 )] [Library(numlist)] [RANDomize] [k(integer 0 )] [ALGorithm(string)] [REPlicate(integer 0 )] [strict] [DIrection(string)] [seed(integer 0 )] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)] [CI(integer 0 )] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0 )] [dt] [reldt] [DTWeight(real 0 )] [DTSave(name)] [oneway] [DETails] [SAVEsmap(string)] [Predictionhorizon(string)] [dot(integer 1 )] [mata] [gpu] [nthreads(integer 0 )] [savemanifold(name)] [idw(real 0 )] [lowmemory] The third subcommand update updates the plugin to its latest version edm update , [develop] [replace] The fourth subcommand version displays the current version number edm version Options Options for explore and xmap subcommands e(numlist ascending) : This option specifies the number of dimensions E used for the main variable in the manifold reconstruction. If a list of numbers is provided, the command will compute results for all numbers specified. The xmap subcommand only supports a single integer as the option whereas the explore subcommand supports the option as a numlist. The default value for E is 2, but in theory E can range from 2 to almost half of the total sample size. The actual E used in the estimation may be different if additional variables are incorporated. A error message is provided if the specified value is out of range. Missing data will limit the maximum E under the default deletion method. tau(integer) : The tau (or \u03c4) option allows researchers to specify the \u2018time delay\u2019, which essentially sorts the data by the multiple \u03c4. This is done by specifying lagged embeddings that take the form: t,t-1\u03c4,\u2026,t-(E-1)\u03c4, where the default is tau(1) (i.e., typical lags). However, if tau(2) is set then every-other t is used to reconstruct the attractor and make predictions\u2014this does not halve the observed sample size because both odd and even t would be used to construct the set of embedding vectors for analysis. This option is helpful when data are oversampled (i.e., spaced too closely in time) and therefore very little new information about a dynamic system is added at each occasion. However, the tau() setting is also useful if different dynamics occur at different times scales, and can be chosen to reflect a researcher\u2019s theory-driven interest in a specific time-scale (e.g., daily instead of hourly). Researchers can evaluate whether \u03c4>1 is required by checking for large autocorrelations in the observed data (e.g., using Stata\u2019s corrgram function). Of course, such a linear measure of association may not work well in nonlinear systems and thus researchers can also check performance by examining \u03c1 and MAE at different values of \u03c4. theta(numlist ascending) : Theta (or \u03b8) is the distance weighting parameter for the local neighbours in the manifold. It is used to detect the nonlinearity of the system in the explore subcommand for S-mapping. Of course, as noted above, for simplex projection and CCM a weight of theta(1) is applied to neighbours based on their distance, which is reflected in the fact that the default value of \u03b8 is 1. However, this can be altered even for simplex projection or CCM (two cases that we do not cover here). Particularly, values for S-mapping to test for improved predictions as they become more local may include the following command: theta(0 .00001 .0001 .001 .005 .01 .05 .1 .5 1 1.5 2 3 4 6 8 10) . k(integer) : This option specifies the number of neighbours used for prediction. When set to 1, only the nearest neighbour is used, but as k increases the next-closest nearest neighbours are included for making predictions. In the case that k is set 0, the number of neighbours used is calculated automatically (typically as k = E + 1 to form a simplex around a target), which is the default value. When k < 0 (e.g., k(-1) ), all possible points in the prediction set are used (i.e., all points in the library are used to reconstruct the manifold and predict target vectors). This latter setting is useful and typically recommended for S-mapping because it allows all points in the library to be used for predictions with the weightings in theta. However, with large datasets this may be computationally burdensome and therefore k(100) or perhaps k(500) may be preferred if T or NT is large. ALGorithm(string) : This option specifies the algorithm used for prediction. If not specified, simplex projection (a locally weighted average) is used. Valid options include simplex and smap, the latter of which is a sequential locally weighted global linear mapping (or S-map as noted previously). In the case of the xmap subcommand where two variables predict each other, the algorithm(smap) invokes something analogous to a distributed lag model with E + 1 predictors (including a constant term c) and, thus, E + 1 locally-weighted coefficients for each predicted observation/target vector\u2014because each predicted observation has its own type of regression done with k neighbours as rows and E + 1 coefficients as columns. As noted below, in this case special options are available to save these coefficients for post-processing but, again, it is not actually a regression model and instead should be seen as a manifold. RANDomize : When splitting the observations into library and prediction sets, by default the oldest observations go into the library set and the newest observations to the prediction set. Though if the randomize option is specified, the data is allocated into the two sets in a random fashion. If the replicate option is specified, then this randomization is enabled automatically. REPlicate(integer) : The explore subcommand uses a random 50/50 split for simplex projection and S-maps, whereas the xmap subcommand selects the observations randomly for library construction if the size of the library L is smaller than the size of all available observations. In these cases, results may be different in each run because the embedding vectors (i.e., the E-dimensional points) used to reconstruct a manifold are chosen at random. The replicate option takes advantages of this to allow repeating the randomization process and calculating results each time. This is akin to a nonparametric bootstrap without replacement, and is commonly used for inference using confidence intervals in EDM (Tsonis et al., 2015; van Nes et al., 2015; Ye et al., 2015b). When replicate is specified, such as replicate(50), mean values and the standard deviations of the results are reported across the 50 runs by default. As we note below, it is possible to save all estimates for post-processing using typical Stata commands such as svmat, allowing the graphing of results or finding percentile-based with the pctile command. PREDICTionsave(variable) : This option allows you to save the edm predictions as a variable, which could be useful for plotting and diagnosis. COPREDICTionsave(variable) : This option allows you to save the copredictions as a variable. You must specify the copredictvar(variables) options for this to work. copredictvar(variable) : This option specifies the variable used for coprediction. A second prediction is run for each configuration of E, library, etc., using the same library set but with a prediction set built from the lagged embedding of this variable. Predictionhorizon(integer) : This option adjusts the default number of observations ahead which we predict. By default, the explore mode predict \u03c4 observations ahead and the xmap mode uses p(0). This parameter can be negative. DETails : By default, only mean values and standard deviations are reported when the replicate option is specified. The details option overrides this behaviour by providing results for each individual run. Irrespective of using this option, all results can be saved for post-processing. CI(integer) : When used with replicate() or crossfold(), this option reports the confidence interval for the mean of the estimates (MAE and/or \u03c1), as well as the percentiles of their distribution. The first row of output labelled \u201cEst. mean CI\u201d reports the estimated confidence interval of the mean \u03c1, assuming that \u03c1 has a normal distribution\u2014estimated as the corrected sample standard deviation (with N-1 in the denominator) divided by the squared root of the number of replications. The reported range can be used to compare mean \u03c1 across different (hyper) parameter values (e.g., different E, \u03b8, or L) using the same datasets as if the sample was the entire population (such that uncertainty is reduced to 0 when the number of replications \u2192\u221e). These intervals can be used to test which (hyper) parameter values best describe a sample, as might be typically used when using crossfold validation methods. The row labelled with \u201cPc (Est.)\u201d follows the same normality assumption and reports the estimated percentile values based on the corrected sample standard deviation of the replicated estimates. The row labelled \u201cPc (Obs.)\u201d reports the actual observed percentile values from the replicated estimates. In both of these latter cases the percentile values offer alternative metrics for comparisons across distributions, which would be more useful for testing typical hypotheses about population differences in estimates across different (hyper) parameter values (e.g., different E, \u03b8, or L), such as testing whether a dynamical system appears to be nonlinear in a population (i.e., testing whether \u03c1 is maximized when \u03b8 > 0). The number specified within the ci() bracket determines the confidence level and the locations of the percentile cut-offs. For example, ci(90) instructs edm to return 90% CI as well as the cut-off values for the 5th and 95th percentile values\u2014because \u03c1 and MAE values cannot or are not expected to take on negative values, we typically prefer one-tailed hypothesis tests and therefore would use ci(90) to get a one-tailed 95% interval. These estimated ranges are also included in the e() return list as a series of scalars with names starting with \u201cub\u201d for upper bound and \u201clb\u201d for lower bound values of the CIs. These return values can be used for further post-processing. seed(integer) : This option specifies the seed used for the random number. In some special cases users may wish to use this in order to keep library and prediction sets the same across simplex projection and S-mapping with a single variable, or across multiple CCM runs with different variables. Note: if set rng has been used to change Stata's random number generation algorithm, then edm will temporarily change it the default 64 bit Mersenne twister internally. strict : When this option is specified, the computation will fail if the requested number of neighboring observations is not present. By default, if not all of the request neighbors are available, the computation will just continue using as many as possible. ALLOWMISSing This option allows observations with missing values to be used in the manifold. Vectors with at least one non-missing values will be used in the manifold construction. Distance computations are adapted to allow missing values when this option is specified. MISSINGdistance(real) : This option allows users to specify the assumed distance between missing values and any values (including missing) when estimating the Euclidean distance of the vector. This enables computations with missing values. The option implies allowmissing. By default, the distance is set to the expected distance of two random draws in a normal distribution, which equals to 2/sqrt(pi) * standard deviation of the mapping variable. EXTRAembed(variables) : This option allows incorporating additional variables into the embedding (multivariate embedding), e.g. extra(z l.z) . Time series lists are unabbreviated here, e.g. extra(L(1/3).z) will be equivalent to extra(L1.z L2.z L3.z) . Normally, lagged versions of the extra variables are not included in the embedding, however the syntax extra(z(e)) includes E lags of z in the embedding. dt : This option allows automatic inclusion of the timestamp differencing in the embedding. There will be E dt variables included for an embedding with E dimensions. By default, the weights used for these additional variables equal to the standard deviation of the main mapping variable divided by the standard deviation of the time difference. This can be overridden by the dtweight() option. dt option will be ignored when running with data with no sampling variation in the time lags. The first dt variable embeds the time of the between the most recent observation and the time of the corresponding target/predictand. reldt : This option, to be read as 'relative dt', is like the 'dt' option above in that it includes E extra variables for an embedding with E dimensions. However the timestamp differences added are not the time between the corresponding observations, but the time of the target/predictand minus the time of the lagged observations. DTWeight(real) : This option specifies the weight used for the timestamp differencing variable. DTSave(variable) : This option allows users to save the internally generated timestamp differencing variable. nthreads(integer) : The number of threads the C++ plugin will use for parallel computations. The default is the number of cores available on the host computer. idw(real) : This parameter is used when xtset indicates that the current dataset is panel data. Then, idw specifies a penalty that is added to the distances between points in the manifold which correspond to observations from different panels. By default idw is 0, so the data from all panels is mixed together and treatly equally. If idw(-1) is set (or any other negative value), then the weight is treated as 'infinity', so neighbours will never be selected which cross the boundaries between panels. Setting idw(-1) with k(-1) means we may use a different number of neighbors for different predictions (i.e. if the panels are unbalanced). lowmemory : It is possible that RAM may be depleted while running edm on large datasets with large values of E. The lowmemory flag directs the plugin to try to save as much space as possible by more efficiently using memory, though for small datasets this will likely slow down the computations by a small but noticeable amount. Besides the shared parameters, edm explore supports the following extra options: CROSSfold(integer) : This option asks the program to run a cross-fold validation of the predicted variables. crossfold(5) indicates a 5-fold cross validation. Note that this cannot be used together with replicate. This option is only available with the explore subcommand. full : When this option is specified, the explore command will use all possible observations in the manifold construction instead of the default 50/50 split. This is effectively the same as leave-one-out cross-validation as the observation itself is not used for the prediction. reportrawe : By default, the program reports the actual E used in the manifold. With this option, the program will only report the number of dimensions constructed from the main variable. Library(numlist ascending) : This option specifies the total library size L used for the manifold reconstruction. Varying the library size is used to estimate the convergence property of the cross-mapping, with a minimum value Lmin = E + 2 and the maximum equal to the total number of observations minus sufficient lags (e.g., in the time-series case without missing data this is Lmax = T + 1 \u2013 E). An error message is given if the L value is beyond the allowed range. To assess the rate of convergence (i.e., the rate at which \u03c1 increases as L grows), the full range of library sizes at small values of L can be used, such as if E = 2 and T = 100, with the setting then perhaps being library(4(1)25 30(5)50 54(15)99) . This option is only available with the xmap subcommand. SAVEsmap(string) : This option allows smap coefficients to be stored in variables with a specified prefix. For example, specifying edm xmap x y, algorithm(smap) savesmap(beta) k(-1) will create a set of new variables such as beta1_b0_rep1. The string prefix (e.g., 'beta') must not be shared with any variables in the dataset, and the option is only valid if the algorithm(smap) is specified. In terms of the saved variables such as beta1_b0_rep1, the first number immediately after the prefix \u2018beta\u2019 is 1 or 2 and indicates which of the two listed variables is treated as the dependent variable in the cross-mapping (i.e., the direction of the mapping). For the edm xmap x y case, variables starting with beta1_ contain coefficients derived from the manifold M_X created using the lags of the first variable \u2018x\u2019 to predict Y, or Y|M_X. This set of variables therefore store the coefficients related to \u2018x\u2019 as an outcome rather than a predictor in CCM. Keep in mind that any Y\u2192X effect associated with the beta1_ prefix is shown as Y|M_X, because the outcome is used to cross-map the predictor, and thus the reported coefficients will be scaled in the opposite direction of a typical regression (because in CCM the outcome variable predicts the cause). To get more familiar regression coefficients (which will be locally weighted), variables starting with beta2_ store the coefficients estimated in the other direction, where the second listed variable \u2018y\u2019 is used for the manifold reconstruction M_Y for the mapping X|M_Y in the \u201cedm xmap x y\u201d case, testing the opposite X\u2192Y effect in CCM, but with reported S-map coefficients that map to a Y\u2192X regression. We appreciate that this may be unintuitive, but because CCM causation is tested by predicting the causal variable with the outcome, to get more familiar regression coefficients requires reversing CCM\u2019s causal direction to a more typical predictor\u2192outcome regression logic. This can be clarified by reverting to the conditional notation such as X|M_Y, which in CCM implies a left-to-right X\u2192Y effect, but for the S-map coefficients will be scaled as a locally-weighted regression in the opposite direction Y\u2192X. Moving on, following the 1 and 2 is the letter b and a number. The numerical labeling scheme generally follows the order of the lag for the main variable and then the order of the extra variables introduced in the case of multivariate embedding. b0 is a special case which records the coefficient of the constant term in the regression. The final term rep1 indicates the coefficients are from the first round of replication (if the replicate() option is not used then there is only one). Finally, the coefficients are saved to match the observation t in the dataset that is being predicted, which allows plotting each of the E estimated coefficients against time and/or the values of the variable being predicted. The variables are also automatically labelled for clarity. This option is only available with the xmap subcommand. DIrection(string) : This option allows users to control whether the cross mapping is calculated bidirectionally or unidirectionally, the latter of which reduces computation times if bidirectional mappings are not required. Valid options include \u201coneway\u201d and \u201cboth\u201d, the latter of which is the default and computes both possible cross-mappings. When oneway is chosen, the first variable listed after the xmap subcommand is treated as the potential dependent variable following the conventions in the regression syntax of Stata such as the\u2018reg\u2019 command, so edm xmap x y, direction(oneway) produces the cross-mapping Y|M_X, which pertains to a Y\u2192X effect. This is consistent with the beta1_ coefficients from the previous savesmap(beta) option. On this point, the direction(oneway) option may be especially useful when an initial \u201cedm xmap x y\u201dprocedure shows convergence only for a cross-mapping Y|M_X, which pertains to a Y\u2192X effect. To save time with large datasets, any follow-up analyses with the algorithm(smap) option can then be conducted with edm xmap x y, algorithm(smap) savesmap(beta) direction(oneway) . To make this easier there is also a simplified oneway option that implies direction(oneway). This option is only available with the xmap subcommand. oneway : This option is equivalent to direction(oneway) Options for update subcommand The update subcommand supports the following options: develop : This option updates the command to its latest development version. The development version usually contains more features but may be less tested compared with the older version distributed on SSC. replace : This option specifies whether you allow the update to override your local ado files. Examples Chicago crime dataset example (included in the auxiliary file) use chicago,clear edm explore temp, e ( 2 / 30 ) edm xmap temp crime edm xmap temp crime, alg(smap) savesmap(beta) e ( 6 ) k( - 1 ) Updates To install the stable version or upgrade directly through Stata: edm update , replace To install the development version directly through Stata: edm update , develop replace","title":"Stata Help File"},{"location":"api/#stata-package-help-file","text":"Note This page simply reproduces, for convenience, the output of typing help edm into the Stata console. The command edm implements a series of tools that can be used for empirical dynamic modeling in Stata. The core algorithm is written in C++ (with a Mata backup) to achieve reasonable execution speed. The command keyword is edm , and should be immediately followed by a subcommand such as explore or xmap. A dataset must be declared as time-series or panel data by the tsset or xtset command prior to using the edm command, and time-series operators including l., f., d., and s. can be used (the last for seasonal differencing).","title":"Stata package help file"},{"location":"api/#syntax","text":"The explore subcommand follows the syntax below and supports one variable for exploration using simplex projection or S-mapping. edm explore variable [if exp], [ e (numlist ascending >= 2 )] [tau(integer 1 )] [theta(numlist ascending)] [k(integer 0 )] [ALGorithm(string)] [REPlicate(integer 0 )] [seed(integer 0 )] [full] [RANDomize] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)] [CROSSfold(integer 0 )] [CI(integer 0 )] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0 )] [dt] [reldt] [DTWeight(real 0 )] [DTSave(name)] [DETails] [reportrawe] [strict] [Predictionhorizon(string)] [dot(integer 1 )] [mata] [gpu] [nthreads(integer 0 )] [savemanifold(name)] [idw(real 0 )] [lowmemory] The second subcommand xmap performs convergent cross-mapping (CCM). The subcommand follows the syntax below and requires two variables to follow immediately after xmap. It shares many of the same options with the explore subcommand although there are some differences given the different purpose of the analysis. edm xmap variables [if exp], [ e (integer 2 )] [tau(integer 1 )] [theta(real 1 )] [Library(numlist)] [RANDomize] [k(integer 0 )] [ALGorithm(string)] [REPlicate(integer 0 )] [strict] [DIrection(string)] [seed(integer 0 )] [PREDICTionsave(name)] [COPREDICTionsave(name)] [copredictvar(string)] [CI(integer 0 )] [EXTRAembed(string)] [ALLOWMISSing] [MISSINGdistance(real 0 )] [dt] [reldt] [DTWeight(real 0 )] [DTSave(name)] [oneway] [DETails] [SAVEsmap(string)] [Predictionhorizon(string)] [dot(integer 1 )] [mata] [gpu] [nthreads(integer 0 )] [savemanifold(name)] [idw(real 0 )] [lowmemory] The third subcommand update updates the plugin to its latest version edm update , [develop] [replace] The fourth subcommand version displays the current version number edm version","title":"Syntax"},{"location":"api/#options","text":"","title":"Options"},{"location":"api/#options-for-explore-and-xmap-subcommands","text":"e(numlist ascending) : This option specifies the number of dimensions E used for the main variable in the manifold reconstruction. If a list of numbers is provided, the command will compute results for all numbers specified. The xmap subcommand only supports a single integer as the option whereas the explore subcommand supports the option as a numlist. The default value for E is 2, but in theory E can range from 2 to almost half of the total sample size. The actual E used in the estimation may be different if additional variables are incorporated. A error message is provided if the specified value is out of range. Missing data will limit the maximum E under the default deletion method. tau(integer) : The tau (or \u03c4) option allows researchers to specify the \u2018time delay\u2019, which essentially sorts the data by the multiple \u03c4. This is done by specifying lagged embeddings that take the form: t,t-1\u03c4,\u2026,t-(E-1)\u03c4, where the default is tau(1) (i.e., typical lags). However, if tau(2) is set then every-other t is used to reconstruct the attractor and make predictions\u2014this does not halve the observed sample size because both odd and even t would be used to construct the set of embedding vectors for analysis. This option is helpful when data are oversampled (i.e., spaced too closely in time) and therefore very little new information about a dynamic system is added at each occasion. However, the tau() setting is also useful if different dynamics occur at different times scales, and can be chosen to reflect a researcher\u2019s theory-driven interest in a specific time-scale (e.g., daily instead of hourly). Researchers can evaluate whether \u03c4>1 is required by checking for large autocorrelations in the observed data (e.g., using Stata\u2019s corrgram function). Of course, such a linear measure of association may not work well in nonlinear systems and thus researchers can also check performance by examining \u03c1 and MAE at different values of \u03c4. theta(numlist ascending) : Theta (or \u03b8) is the distance weighting parameter for the local neighbours in the manifold. It is used to detect the nonlinearity of the system in the explore subcommand for S-mapping. Of course, as noted above, for simplex projection and CCM a weight of theta(1) is applied to neighbours based on their distance, which is reflected in the fact that the default value of \u03b8 is 1. However, this can be altered even for simplex projection or CCM (two cases that we do not cover here). Particularly, values for S-mapping to test for improved predictions as they become more local may include the following command: theta(0 .00001 .0001 .001 .005 .01 .05 .1 .5 1 1.5 2 3 4 6 8 10) . k(integer) : This option specifies the number of neighbours used for prediction. When set to 1, only the nearest neighbour is used, but as k increases the next-closest nearest neighbours are included for making predictions. In the case that k is set 0, the number of neighbours used is calculated automatically (typically as k = E + 1 to form a simplex around a target), which is the default value. When k < 0 (e.g., k(-1) ), all possible points in the prediction set are used (i.e., all points in the library are used to reconstruct the manifold and predict target vectors). This latter setting is useful and typically recommended for S-mapping because it allows all points in the library to be used for predictions with the weightings in theta. However, with large datasets this may be computationally burdensome and therefore k(100) or perhaps k(500) may be preferred if T or NT is large. ALGorithm(string) : This option specifies the algorithm used for prediction. If not specified, simplex projection (a locally weighted average) is used. Valid options include simplex and smap, the latter of which is a sequential locally weighted global linear mapping (or S-map as noted previously). In the case of the xmap subcommand where two variables predict each other, the algorithm(smap) invokes something analogous to a distributed lag model with E + 1 predictors (including a constant term c) and, thus, E + 1 locally-weighted coefficients for each predicted observation/target vector\u2014because each predicted observation has its own type of regression done with k neighbours as rows and E + 1 coefficients as columns. As noted below, in this case special options are available to save these coefficients for post-processing but, again, it is not actually a regression model and instead should be seen as a manifold. RANDomize : When splitting the observations into library and prediction sets, by default the oldest observations go into the library set and the newest observations to the prediction set. Though if the randomize option is specified, the data is allocated into the two sets in a random fashion. If the replicate option is specified, then this randomization is enabled automatically. REPlicate(integer) : The explore subcommand uses a random 50/50 split for simplex projection and S-maps, whereas the xmap subcommand selects the observations randomly for library construction if the size of the library L is smaller than the size of all available observations. In these cases, results may be different in each run because the embedding vectors (i.e., the E-dimensional points) used to reconstruct a manifold are chosen at random. The replicate option takes advantages of this to allow repeating the randomization process and calculating results each time. This is akin to a nonparametric bootstrap without replacement, and is commonly used for inference using confidence intervals in EDM (Tsonis et al., 2015; van Nes et al., 2015; Ye et al., 2015b). When replicate is specified, such as replicate(50), mean values and the standard deviations of the results are reported across the 50 runs by default. As we note below, it is possible to save all estimates for post-processing using typical Stata commands such as svmat, allowing the graphing of results or finding percentile-based with the pctile command. PREDICTionsave(variable) : This option allows you to save the edm predictions as a variable, which could be useful for plotting and diagnosis. COPREDICTionsave(variable) : This option allows you to save the copredictions as a variable. You must specify the copredictvar(variables) options for this to work. copredictvar(variable) : This option specifies the variable used for coprediction. A second prediction is run for each configuration of E, library, etc., using the same library set but with a prediction set built from the lagged embedding of this variable. Predictionhorizon(integer) : This option adjusts the default number of observations ahead which we predict. By default, the explore mode predict \u03c4 observations ahead and the xmap mode uses p(0). This parameter can be negative. DETails : By default, only mean values and standard deviations are reported when the replicate option is specified. The details option overrides this behaviour by providing results for each individual run. Irrespective of using this option, all results can be saved for post-processing. CI(integer) : When used with replicate() or crossfold(), this option reports the confidence interval for the mean of the estimates (MAE and/or \u03c1), as well as the percentiles of their distribution. The first row of output labelled \u201cEst. mean CI\u201d reports the estimated confidence interval of the mean \u03c1, assuming that \u03c1 has a normal distribution\u2014estimated as the corrected sample standard deviation (with N-1 in the denominator) divided by the squared root of the number of replications. The reported range can be used to compare mean \u03c1 across different (hyper) parameter values (e.g., different E, \u03b8, or L) using the same datasets as if the sample was the entire population (such that uncertainty is reduced to 0 when the number of replications \u2192\u221e). These intervals can be used to test which (hyper) parameter values best describe a sample, as might be typically used when using crossfold validation methods. The row labelled with \u201cPc (Est.)\u201d follows the same normality assumption and reports the estimated percentile values based on the corrected sample standard deviation of the replicated estimates. The row labelled \u201cPc (Obs.)\u201d reports the actual observed percentile values from the replicated estimates. In both of these latter cases the percentile values offer alternative metrics for comparisons across distributions, which would be more useful for testing typical hypotheses about population differences in estimates across different (hyper) parameter values (e.g., different E, \u03b8, or L), such as testing whether a dynamical system appears to be nonlinear in a population (i.e., testing whether \u03c1 is maximized when \u03b8 > 0). The number specified within the ci() bracket determines the confidence level and the locations of the percentile cut-offs. For example, ci(90) instructs edm to return 90% CI as well as the cut-off values for the 5th and 95th percentile values\u2014because \u03c1 and MAE values cannot or are not expected to take on negative values, we typically prefer one-tailed hypothesis tests and therefore would use ci(90) to get a one-tailed 95% interval. These estimated ranges are also included in the e() return list as a series of scalars with names starting with \u201cub\u201d for upper bound and \u201clb\u201d for lower bound values of the CIs. These return values can be used for further post-processing. seed(integer) : This option specifies the seed used for the random number. In some special cases users may wish to use this in order to keep library and prediction sets the same across simplex projection and S-mapping with a single variable, or across multiple CCM runs with different variables. Note: if set rng has been used to change Stata's random number generation algorithm, then edm will temporarily change it the default 64 bit Mersenne twister internally. strict : When this option is specified, the computation will fail if the requested number of neighboring observations is not present. By default, if not all of the request neighbors are available, the computation will just continue using as many as possible. ALLOWMISSing This option allows observations with missing values to be used in the manifold. Vectors with at least one non-missing values will be used in the manifold construction. Distance computations are adapted to allow missing values when this option is specified. MISSINGdistance(real) : This option allows users to specify the assumed distance between missing values and any values (including missing) when estimating the Euclidean distance of the vector. This enables computations with missing values. The option implies allowmissing. By default, the distance is set to the expected distance of two random draws in a normal distribution, which equals to 2/sqrt(pi) * standard deviation of the mapping variable. EXTRAembed(variables) : This option allows incorporating additional variables into the embedding (multivariate embedding), e.g. extra(z l.z) . Time series lists are unabbreviated here, e.g. extra(L(1/3).z) will be equivalent to extra(L1.z L2.z L3.z) . Normally, lagged versions of the extra variables are not included in the embedding, however the syntax extra(z(e)) includes E lags of z in the embedding. dt : This option allows automatic inclusion of the timestamp differencing in the embedding. There will be E dt variables included for an embedding with E dimensions. By default, the weights used for these additional variables equal to the standard deviation of the main mapping variable divided by the standard deviation of the time difference. This can be overridden by the dtweight() option. dt option will be ignored when running with data with no sampling variation in the time lags. The first dt variable embeds the time of the between the most recent observation and the time of the corresponding target/predictand. reldt : This option, to be read as 'relative dt', is like the 'dt' option above in that it includes E extra variables for an embedding with E dimensions. However the timestamp differences added are not the time between the corresponding observations, but the time of the target/predictand minus the time of the lagged observations. DTWeight(real) : This option specifies the weight used for the timestamp differencing variable. DTSave(variable) : This option allows users to save the internally generated timestamp differencing variable. nthreads(integer) : The number of threads the C++ plugin will use for parallel computations. The default is the number of cores available on the host computer. idw(real) : This parameter is used when xtset indicates that the current dataset is panel data. Then, idw specifies a penalty that is added to the distances between points in the manifold which correspond to observations from different panels. By default idw is 0, so the data from all panels is mixed together and treatly equally. If idw(-1) is set (or any other negative value), then the weight is treated as 'infinity', so neighbours will never be selected which cross the boundaries between panels. Setting idw(-1) with k(-1) means we may use a different number of neighbors for different predictions (i.e. if the panels are unbalanced). lowmemory : It is possible that RAM may be depleted while running edm on large datasets with large values of E. The lowmemory flag directs the plugin to try to save as much space as possible by more efficiently using memory, though for small datasets this will likely slow down the computations by a small but noticeable amount. Besides the shared parameters, edm explore supports the following extra options: CROSSfold(integer) : This option asks the program to run a cross-fold validation of the predicted variables. crossfold(5) indicates a 5-fold cross validation. Note that this cannot be used together with replicate. This option is only available with the explore subcommand. full : When this option is specified, the explore command will use all possible observations in the manifold construction instead of the default 50/50 split. This is effectively the same as leave-one-out cross-validation as the observation itself is not used for the prediction. reportrawe : By default, the program reports the actual E used in the manifold. With this option, the program will only report the number of dimensions constructed from the main variable. Library(numlist ascending) : This option specifies the total library size L used for the manifold reconstruction. Varying the library size is used to estimate the convergence property of the cross-mapping, with a minimum value Lmin = E + 2 and the maximum equal to the total number of observations minus sufficient lags (e.g., in the time-series case without missing data this is Lmax = T + 1 \u2013 E). An error message is given if the L value is beyond the allowed range. To assess the rate of convergence (i.e., the rate at which \u03c1 increases as L grows), the full range of library sizes at small values of L can be used, such as if E = 2 and T = 100, with the setting then perhaps being library(4(1)25 30(5)50 54(15)99) . This option is only available with the xmap subcommand. SAVEsmap(string) : This option allows smap coefficients to be stored in variables with a specified prefix. For example, specifying edm xmap x y, algorithm(smap) savesmap(beta) k(-1) will create a set of new variables such as beta1_b0_rep1. The string prefix (e.g., 'beta') must not be shared with any variables in the dataset, and the option is only valid if the algorithm(smap) is specified. In terms of the saved variables such as beta1_b0_rep1, the first number immediately after the prefix \u2018beta\u2019 is 1 or 2 and indicates which of the two listed variables is treated as the dependent variable in the cross-mapping (i.e., the direction of the mapping). For the edm xmap x y case, variables starting with beta1_ contain coefficients derived from the manifold M_X created using the lags of the first variable \u2018x\u2019 to predict Y, or Y|M_X. This set of variables therefore store the coefficients related to \u2018x\u2019 as an outcome rather than a predictor in CCM. Keep in mind that any Y\u2192X effect associated with the beta1_ prefix is shown as Y|M_X, because the outcome is used to cross-map the predictor, and thus the reported coefficients will be scaled in the opposite direction of a typical regression (because in CCM the outcome variable predicts the cause). To get more familiar regression coefficients (which will be locally weighted), variables starting with beta2_ store the coefficients estimated in the other direction, where the second listed variable \u2018y\u2019 is used for the manifold reconstruction M_Y for the mapping X|M_Y in the \u201cedm xmap x y\u201d case, testing the opposite X\u2192Y effect in CCM, but with reported S-map coefficients that map to a Y\u2192X regression. We appreciate that this may be unintuitive, but because CCM causation is tested by predicting the causal variable with the outcome, to get more familiar regression coefficients requires reversing CCM\u2019s causal direction to a more typical predictor\u2192outcome regression logic. This can be clarified by reverting to the conditional notation such as X|M_Y, which in CCM implies a left-to-right X\u2192Y effect, but for the S-map coefficients will be scaled as a locally-weighted regression in the opposite direction Y\u2192X. Moving on, following the 1 and 2 is the letter b and a number. The numerical labeling scheme generally follows the order of the lag for the main variable and then the order of the extra variables introduced in the case of multivariate embedding. b0 is a special case which records the coefficient of the constant term in the regression. The final term rep1 indicates the coefficients are from the first round of replication (if the replicate() option is not used then there is only one). Finally, the coefficients are saved to match the observation t in the dataset that is being predicted, which allows plotting each of the E estimated coefficients against time and/or the values of the variable being predicted. The variables are also automatically labelled for clarity. This option is only available with the xmap subcommand. DIrection(string) : This option allows users to control whether the cross mapping is calculated bidirectionally or unidirectionally, the latter of which reduces computation times if bidirectional mappings are not required. Valid options include \u201coneway\u201d and \u201cboth\u201d, the latter of which is the default and computes both possible cross-mappings. When oneway is chosen, the first variable listed after the xmap subcommand is treated as the potential dependent variable following the conventions in the regression syntax of Stata such as the\u2018reg\u2019 command, so edm xmap x y, direction(oneway) produces the cross-mapping Y|M_X, which pertains to a Y\u2192X effect. This is consistent with the beta1_ coefficients from the previous savesmap(beta) option. On this point, the direction(oneway) option may be especially useful when an initial \u201cedm xmap x y\u201dprocedure shows convergence only for a cross-mapping Y|M_X, which pertains to a Y\u2192X effect. To save time with large datasets, any follow-up analyses with the algorithm(smap) option can then be conducted with edm xmap x y, algorithm(smap) savesmap(beta) direction(oneway) . To make this easier there is also a simplified oneway option that implies direction(oneway). This option is only available with the xmap subcommand. oneway : This option is equivalent to direction(oneway)","title":"Options for explore and xmap subcommands"},{"location":"api/#options-for-update-subcommand","text":"The update subcommand supports the following options: develop : This option updates the command to its latest development version. The development version usually contains more features but may be less tested compared with the older version distributed on SSC. replace : This option specifies whether you allow the update to override your local ado files.","title":"Options for update subcommand"},{"location":"api/#examples","text":"Chicago crime dataset example (included in the auxiliary file) use chicago,clear edm explore temp, e ( 2 / 30 ) edm xmap temp crime edm xmap temp crime, alg(smap) savesmap(beta) e ( 6 ) k( - 1 )","title":"Examples"},{"location":"api/#updates","text":"To install the stable version or upgrade directly through Stata: edm update , replace To install the development version directly through Stata: edm update , develop replace","title":"Updates"},{"location":"automated-workflows/","text":"Automated workflow scripts Bug There are currently known bugs in the following do scripts. Until we can fix them, we suggest that you either: i) directly use the edm commands as per the examples on this site, ii) use the scripts as an inspiration for your do files, or iii) help us track down the bugs with a pull request to https://github.com/EDM-Developers/EDM/ . Linked below are some Stata scripts using edm which automate some common analyses. They provide some early stage prototypes allowing automated Simplex Projections, S-maps, Coprediction, and Convergent Cross Mapping as well as relevant hypothesis tests: edm Plugin - Automated time-series analysis (N=1 case) edm Plugin - Automated multi-spatial analysis (N>1 panel data case) edm Plugin - Automated multiple-edm analysis (N>1 panel data case)","title":"Automated workflows"},{"location":"automated-workflows/#automated-workflow-scripts","text":"Bug There are currently known bugs in the following do scripts. Until we can fix them, we suggest that you either: i) directly use the edm commands as per the examples on this site, ii) use the scripts as an inspiration for your do files, or iii) help us track down the bugs with a pull request to https://github.com/EDM-Developers/EDM/ . Linked below are some Stata scripts using edm which automate some common analyses. They provide some early stage prototypes allowing automated Simplex Projections, S-maps, Coprediction, and Convergent Cross Mapping as well as relevant hypothesis tests: edm Plugin - Automated time-series analysis (N=1 case) edm Plugin - Automated multi-spatial analysis (N>1 panel data case) edm Plugin - Automated multiple-edm analysis (N>1 panel data case)","title":"Automated workflow scripts"},{"location":"coprediction/","text":"Coprediction What does copredict do in explore mode? Imagine that we use the command: edm explore x, copredictvar(z) copredict(out) This will first do a normal edm explore x operation, then it will perform a second set of copredictions . This brings in a second time series \\(z\\) , and specifies that the predictions made in copredict mode should be stored in the Stata variable named out . For the following, let's first set the general manifold parameters. Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) In coprediction mode, the training set will include the entirety of the \\(M_x\\) manifold and its projections: In copredict mode the most significant difference is that we change \\(\\mathscr{P}\\) to be the \\(M_z\\) manifold for the \\(z\\) time series and \\(y^{\\mathscr{P}}\\) to: The rest of the simplex procedure is the same as before: \\[ \\begin{aligned} \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{ \\text{Based on } z } & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }{ \\text{Based on } z} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\] What does copredict do in xmap mode? Imagine that we use the command: edm xmap u v, oneway copredictvar(w) copredict(out) Now we combine three different time series to create the predictions in the out Stata variable. In this case, the training set contains all the points in \\(M_u\\) : The main change in coprediction is the prediction set and the targets are based on the new \\(w\\) time series: Finally, the simplex prediction steps are the same, with: \\[ \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{\\text{Based on } w} \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }_{ \\text{Based on } w } \\underset{\\small \\text{Find neighbours in}}{\\Rightarrow} \\underbrace{ \\mathscr{L} }_{\\text{Based on } u} \\underset{\\small \\text{Matches}}{\\Rightarrow} \\underbrace{ \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} }_{\\text{Based on } v} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\underbrace{ \\hat{y}_i^{\\mathscr{P}} }_{\\text{Based on } v} \\]","title":"Coprediction"},{"location":"coprediction/#coprediction","text":"","title":"Coprediction"},{"location":"coprediction/#what-does-copredict-do-in-explore-mode","text":"Imagine that we use the command: edm explore x, copredictvar(z) copredict(out) This will first do a normal edm explore x operation, then it will perform a second set of copredictions . This brings in a second time series \\(z\\) , and specifies that the predictions made in copredict mode should be stored in the Stata variable named out . For the following, let's first set the general manifold parameters. Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) In coprediction mode, the training set will include the entirety of the \\(M_x\\) manifold and its projections: In copredict mode the most significant difference is that we change \\(\\mathscr{P}\\) to be the \\(M_z\\) manifold for the \\(z\\) time series and \\(y^{\\mathscr{P}}\\) to: The rest of the simplex procedure is the same as before: \\[ \\begin{aligned} \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{ \\text{Based on } z } & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }{ \\text{Based on } z} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\]","title":"What does copredict do in explore mode?"},{"location":"coprediction/#what-does-copredict-do-in-xmap-mode","text":"Imagine that we use the command: edm xmap u v, oneway copredictvar(w) copredict(out) Now we combine three different time series to create the predictions in the out Stata variable. In this case, the training set contains all the points in \\(M_u\\) : The main change in coprediction is the prediction set and the targets are based on the new \\(w\\) time series: Finally, the simplex prediction steps are the same, with: \\[ \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{\\text{Based on } w} \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }_{ \\text{Based on } w } \\underset{\\small \\text{Find neighbours in}}{\\Rightarrow} \\underbrace{ \\mathscr{L} }_{\\text{Based on } u} \\underset{\\small \\text{Matches}}{\\Rightarrow} \\underbrace{ \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} }_{\\text{Based on } v} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\underbrace{ \\hat{y}_i^{\\mathscr{P}} }_{\\text{Based on } v} \\]","title":"What does copredict do in xmap mode?"},{"location":"explore/","text":"What does edm explore x do? First split into library and prediction sets Firstly, the manifold \\(M_x\\) is split into two parts, called the library set denoted \\(\\mathscr{L}\\) and the prediction set denoted \\(\\mathscr{P}\\) . By default, we take the points of the \\(M_x\\) manifold and assign the first half of them to \\(\\mathscr{L}\\) and the second half to \\(\\mathscr{P}\\) . Note In the default case, the same point doesn't appear in both \\(\\mathscr{L}\\) and \\(\\mathscr{P}\\) , though given other options then the same point may appear in both sets. Starting with the time-delayed embedding of \\(x\\) . Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with the selected \\(E\\) and \\(\\tau\\) is the manifold: Then we take the first half of the points to create the library set, which leaves the remaining points to create the prediction set. In that case, the library set is and the prediction set is It will help to introduce a notation to refer to a specific point in these sets based on its row number. E.g. in the example above, the first point in the library set takes the value: More generally \\(\\mathscr{L}_{i}\\) refers to the \\(i\\) th point in \\(\\mathscr{L}\\) and similarly \\(\\mathscr{P}_{j}\\) refers to the \\(j\\) th point in \\(\\mathscr{P}\\) . Next, look at the future values of each point Each point on the manifold refers to a small trajectory of a time series, and for each point we look \\(p\\) observations into the future of the time series. Choose a value for \\(p\\) So if we take the first point of the prediction set \\(\\mathscr{P}_{1}\\) and say that \\(y_1^{\\mathscr{P}}\\) is the value it takes \\(p\\) observations in the future, we get: This \\(p\\) may be thought of as the prediction horizon , and in explore mode is defaults to \\(\\tau\\) and in xmap mode it defaults to 0. Our \\(p\\) versus the \\(T_p\\) which is common in the literature In the literature, instead of measuring the number of observations \\(p\\) ahead, authors normally use the value \\(T_p\\) to denote the amount of time this corresponds to. When data is regularly sampled (e.g. \\(t_i = i\\) ) then there is no difference (e.g. \\(T_p = p\\) ), however for irregularly sampled data the actual time difference may be different for each prediction. In the training set, this means each point of \\(\\mathscr{L}\\) matches the corresponding value in \\(y^{\\,\\mathscr{L}}\\) : Similarly, for the prediction set: We may refer to elements of the \\(y^{\\mathscr{L}}\\) vector as projections as they come about by taking the \\(x\\) time series and projecting it into the future by \\(p\\) observations. What does edm explore x predict? When running edm explore x , we pretend that we don't know the values in \\(y^{\\mathscr{P}}\\) and that we want to predict them given we know \\(\\mathscr{P}\\) , \\(\\mathscr{L}\\) and \\(y^{\\,\\mathscr{L}}\\) . The first prediction is to try to find the value of \\(y_1^{\\mathscr{P}}\\) given the corresponding point \\(\\mathscr{P}_1\\) : The terminology we use is that \\(y_1^{\\mathscr{P}}\\) is the target and the point \\(\\mathscr{P}_1\\) is the predictee . Looking over all the points in \\(\\mathscr{L}\\) , we find the indices of the \\(k\\) points which are the most similar to \\(\\mathscr{P}_{1}\\) . Let's pretend we have \\(k=2\\) and the most similar points are \\(\\mathscr{L}_{3}\\) and \\(\\mathscr{L}_{5}\\) . We will choose the notation \\(\\mathcal{NN}_k(1) = \\{ 3, 5 \\}\\) to describe this set of \\(k\\) nearest neighbours of \\(\\mathscr{P}_{1}\\) . Using the simplex algorithm Then, if we have chosen the algorithm to be the simplex algorithm, we predict that \\[ y_{1}^{\\mathscr{P}} \\approx \\hat{y}_1^{\\mathscr{P}} := w_1 \\times y_{3}^{\\,\\mathscr{L}} + w_2 \\times y_{5}^{\\,\\mathscr{L}} \\] where \\(w_1\\) and \\(w_2\\) are some weights with add up to 1. Basically we are predicting that \\(y_1^{\\mathscr{P}}\\) is a weighted average of the points \\(\\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(1)}\\) . To summarise the whole simplex procedure: \\[ \\begin{aligned} \\text{For target }y_i^{\\mathscr{P}} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\mathscr{P}_{i} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\] Using the S-map algorithm If however we chose algorithm to be the S-map algorithm, then we predict \\[ y_{1}^{\\mathscr{P}} \\approx \\hat{y}_1^{\\mathscr{P}} := \\sum_{j=1}^E w_{1,j} \\times \\mathscr{P}_{1j} \\] where the \\(\\{ w_{1,j} \\}_{j=1,\\cdots,E}\\) weights are calculated by solving a linear system based on the points in \\(\\{ \\mathscr{L}_j \\}_{j \\in \\mathcal{NN}_k(1)}\\) and \\(\\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(1)}\\) . Given the specific in this example, the prediction would look like: To summarise the whole S-map procedure: \\[ \\begin{aligned} \\text{For target }y_i^{\\mathscr{P}} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\mathscr{P}_{i} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ \\mathscr{L}_j, y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Calculate}}{\\Rightarrow} \\{ w_{i,j} \\}_{j=1,\\ldots,E} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\] Assessing the prediction quality We calculate the \\(\\hat{y}_i^{\\mathscr{P}}\\) predictions for each target in the prediction set (so \\(i = 1, \\dots, |\\mathscr{P}|\\) ), and store the predictions in a vector \\(\\hat{y}^{\\mathscr{P}}\\) . As we observe the true value of \\(y_i^{\\mathscr{P}}\\) for most (if not all) of the targets in the prediction set, we can compare our \\(\\hat{y}_i^{\\mathscr{P}}\\) predictions to the observed values. We assess the quality of the predictions using either the correlation \\[ \\rho := \\text{Correlation}(y^{\\mathscr{P}} , \\hat{y}^{\\mathscr{P}}) \\] or using the mean absolute error \\[ \\text{MAE} := \\frac{1}{| \\mathscr{P} |} \\sum_{i=1}^{| \\mathscr{P} |} | y_i^{\\mathscr{P}} - \\hat{y}_i^{\\mathscr{P}} | . \\]","title":"The explore subcommand"},{"location":"explore/#what-does-edm-explore-x-do","text":"","title":"What does edm explore x do?"},{"location":"explore/#first-split-into-library-and-prediction-sets","text":"Firstly, the manifold \\(M_x\\) is split into two parts, called the library set denoted \\(\\mathscr{L}\\) and the prediction set denoted \\(\\mathscr{P}\\) . By default, we take the points of the \\(M_x\\) manifold and assign the first half of them to \\(\\mathscr{L}\\) and the second half to \\(\\mathscr{P}\\) . Note In the default case, the same point doesn't appear in both \\(\\mathscr{L}\\) and \\(\\mathscr{P}\\) , though given other options then the same point may appear in both sets. Starting with the time-delayed embedding of \\(x\\) . Choose the number of observations Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with the selected \\(E\\) and \\(\\tau\\) is the manifold: Then we take the first half of the points to create the library set, which leaves the remaining points to create the prediction set. In that case, the library set is and the prediction set is It will help to introduce a notation to refer to a specific point in these sets based on its row number. E.g. in the example above, the first point in the library set takes the value: More generally \\(\\mathscr{L}_{i}\\) refers to the \\(i\\) th point in \\(\\mathscr{L}\\) and similarly \\(\\mathscr{P}_{j}\\) refers to the \\(j\\) th point in \\(\\mathscr{P}\\) .","title":"First split into library and prediction sets"},{"location":"explore/#next-look-at-the-future-values-of-each-point","text":"Each point on the manifold refers to a small trajectory of a time series, and for each point we look \\(p\\) observations into the future of the time series. Choose a value for \\(p\\) So if we take the first point of the prediction set \\(\\mathscr{P}_{1}\\) and say that \\(y_1^{\\mathscr{P}}\\) is the value it takes \\(p\\) observations in the future, we get: This \\(p\\) may be thought of as the prediction horizon , and in explore mode is defaults to \\(\\tau\\) and in xmap mode it defaults to 0. Our \\(p\\) versus the \\(T_p\\) which is common in the literature In the literature, instead of measuring the number of observations \\(p\\) ahead, authors normally use the value \\(T_p\\) to denote the amount of time this corresponds to. When data is regularly sampled (e.g. \\(t_i = i\\) ) then there is no difference (e.g. \\(T_p = p\\) ), however for irregularly sampled data the actual time difference may be different for each prediction. In the training set, this means each point of \\(\\mathscr{L}\\) matches the corresponding value in \\(y^{\\,\\mathscr{L}}\\) : Similarly, for the prediction set: We may refer to elements of the \\(y^{\\mathscr{L}}\\) vector as projections as they come about by taking the \\(x\\) time series and projecting it into the future by \\(p\\) observations.","title":"Next, look at the future values of each point"},{"location":"explore/#what-does-edm-explore-x-predict","text":"When running edm explore x , we pretend that we don't know the values in \\(y^{\\mathscr{P}}\\) and that we want to predict them given we know \\(\\mathscr{P}\\) , \\(\\mathscr{L}\\) and \\(y^{\\,\\mathscr{L}}\\) . The first prediction is to try to find the value of \\(y_1^{\\mathscr{P}}\\) given the corresponding point \\(\\mathscr{P}_1\\) : The terminology we use is that \\(y_1^{\\mathscr{P}}\\) is the target and the point \\(\\mathscr{P}_1\\) is the predictee . Looking over all the points in \\(\\mathscr{L}\\) , we find the indices of the \\(k\\) points which are the most similar to \\(\\mathscr{P}_{1}\\) . Let's pretend we have \\(k=2\\) and the most similar points are \\(\\mathscr{L}_{3}\\) and \\(\\mathscr{L}_{5}\\) . We will choose the notation \\(\\mathcal{NN}_k(1) = \\{ 3, 5 \\}\\) to describe this set of \\(k\\) nearest neighbours of \\(\\mathscr{P}_{1}\\) .","title":"What does edm explore x predict?"},{"location":"explore/#using-the-simplex-algorithm","text":"Then, if we have chosen the algorithm to be the simplex algorithm, we predict that \\[ y_{1}^{\\mathscr{P}} \\approx \\hat{y}_1^{\\mathscr{P}} := w_1 \\times y_{3}^{\\,\\mathscr{L}} + w_2 \\times y_{5}^{\\,\\mathscr{L}} \\] where \\(w_1\\) and \\(w_2\\) are some weights with add up to 1. Basically we are predicting that \\(y_1^{\\mathscr{P}}\\) is a weighted average of the points \\(\\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(1)}\\) . To summarise the whole simplex procedure: \\[ \\begin{aligned} \\text{For target }y_i^{\\mathscr{P}} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\mathscr{P}_{i} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\]","title":"Using the simplex algorithm"},{"location":"explore/#using-the-s-map-algorithm","text":"If however we chose algorithm to be the S-map algorithm, then we predict \\[ y_{1}^{\\mathscr{P}} \\approx \\hat{y}_1^{\\mathscr{P}} := \\sum_{j=1}^E w_{1,j} \\times \\mathscr{P}_{1j} \\] where the \\(\\{ w_{1,j} \\}_{j=1,\\cdots,E}\\) weights are calculated by solving a linear system based on the points in \\(\\{ \\mathscr{L}_j \\}_{j \\in \\mathcal{NN}_k(1)}\\) and \\(\\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(1)}\\) . Given the specific in this example, the prediction would look like: To summarise the whole S-map procedure: \\[ \\begin{aligned} \\text{For target }y_i^{\\mathscr{P}} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\mathscr{P}_{i} \\underset{\\small \\text{Find neighbours in } \\mathscr{L}}{\\Rightarrow} \\mathcal{NN}_k(i) \\\\ &\\,\\,\\,\\, \\underset{\\small \\text{Extracts}}{\\Rightarrow} \\{ \\mathscr{L}_j, y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} \\underset{\\small \\text{Calculate}}{\\Rightarrow} \\{ w_{i,j} \\}_{j=1,\\ldots,E} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\hat{y}_i^{\\mathscr{P}} \\end{aligned} \\]","title":"Using the S-map algorithm"},{"location":"explore/#assessing-the-prediction-quality","text":"We calculate the \\(\\hat{y}_i^{\\mathscr{P}}\\) predictions for each target in the prediction set (so \\(i = 1, \\dots, |\\mathscr{P}|\\) ), and store the predictions in a vector \\(\\hat{y}^{\\mathscr{P}}\\) . As we observe the true value of \\(y_i^{\\mathscr{P}}\\) for most (if not all) of the targets in the prediction set, we can compare our \\(\\hat{y}_i^{\\mathscr{P}}\\) predictions to the observed values. We assess the quality of the predictions using either the correlation \\[ \\rho := \\text{Correlation}(y^{\\mathscr{P}} , \\hat{y}^{\\mathscr{P}}) \\] or using the mean absolute error \\[ \\text{MAE} := \\frac{1}{| \\mathscr{P} |} \\sum_{i=1}^{| \\mathscr{P} |} | y_i^{\\mathscr{P}} - \\hat{y}_i^{\\mathscr{P}} | . \\]","title":"Assessing the prediction quality"},{"location":"gpu/","text":"GPU acceleration Setup The edm commands normally run C++ code which run on multiple threads of a CPU, however it is possible to use the gpu option to move the heavy computation to an attached GPU. To use this GPU acceleration, first make sure you are using a Windows or Linux machine which has an NVIDIA graphics card attached (preferably one of either Pascal, Volta, or Turing generations, though others may work if compiled locally). Make sure your graphics drivers are installed (& relatively up-to-date), and install CUDA version 11 or above. Next, install ArrayFire . Finally, make sure the latest development version of the edm package is installed by running ssc install edm // If no version of edm has been installed edm update , development replace // To get the most up-to-date version and your machine should be ready to run the GPU-accelerated. Usage That means, any edm explore or edm xmap command can use the gpu option and the processing will use the attached GPU. E.g. edm explore x, gpu edm xmap x y, gpu Alternatively, if you set the global EDM_GPU = 1 then all subsequent edm commands will also run on the GPU, e.g. global EDM_GPU = 1 edm explore x edm xmap x y Contributors The GPU code was contributed by the ArrayFire engineers: Pradeep Garigipati, Umar Arshad, John Melonakos. This collaboration was funded by the ARC Discovery Project DP200100219.","title":"GPU Acceleration"},{"location":"gpu/#gpu-acceleration","text":"","title":"GPU acceleration"},{"location":"gpu/#setup","text":"The edm commands normally run C++ code which run on multiple threads of a CPU, however it is possible to use the gpu option to move the heavy computation to an attached GPU. To use this GPU acceleration, first make sure you are using a Windows or Linux machine which has an NVIDIA graphics card attached (preferably one of either Pascal, Volta, or Turing generations, though others may work if compiled locally). Make sure your graphics drivers are installed (& relatively up-to-date), and install CUDA version 11 or above. Next, install ArrayFire . Finally, make sure the latest development version of the edm package is installed by running ssc install edm // If no version of edm has been installed edm update , development replace // To get the most up-to-date version and your machine should be ready to run the GPU-accelerated.","title":"Setup"},{"location":"gpu/#usage","text":"That means, any edm explore or edm xmap command can use the gpu option and the processing will use the attached GPU. E.g. edm explore x, gpu edm xmap x y, gpu Alternatively, if you set the global EDM_GPU = 1 then all subsequent edm commands will also run on the GPU, e.g. global EDM_GPU = 1 edm explore x edm xmap x y","title":"Usage"},{"location":"gpu/#contributors","text":"The GPU code was contributed by the ArrayFire engineers: Pradeep Garigipati, Umar Arshad, John Melonakos. This collaboration was funded by the ARC Discovery Project DP200100219.","title":"Contributors"},{"location":"missing-data/","text":"Missing data To explain how the package handles missing data given different options, it is easiest to work by example. Let's say we have the following time series and NA represents a missing value: \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 Let's also fix \\(E = 2\\) , \\(\\tau = 1\\) and \\(p = 1\\) for these examples. Here we have one obviously missing value for \\(x\\) at time 3. However, there are some hidden missing values also. By default, the package will assume that your data was measured at a regular time interval and will insert missing values as necessary to create a regular grid. For example, the above time series will be treated as if it were sampled every half time unit. So, when creating the \\(E=2\\) manifold it will \\(t\\) \\(x_t\\) 1.0 11 1.5 NA 2.0 NA 2.5 12 3.0 NA 3.5 NA 4.0 NA 4.5 14 5.0 15 5.5 NA 6.0 16 The manifold of \\(x\\) and it's projections \\(y\\) will have missing values in them: \\[ M_x = \\left[\\begin{array}{cc} 11 & \\text{NA} \\\\ %\\text{NA} & 11 \\\\ %\\text{NA} & \\text{NA} \\\\ 12 & \\text{NA} \\\\ \\text{NA} & 12 \\\\ %\\text{NA} & \\text{NA} \\\\ %\\text{NA} & \\text{NA} \\\\ 14 & \\text{NA} \\\\ 15 & 14 \\\\ %\\text{NA} & 15 \\\\ 16 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} \\text{NA} \\\\ %\\text{NA} \\\\ %12 \\\\ \\text{NA} \\\\ \\text{NA} \\\\ %\\text{NA} \\\\ %14 \\\\ 15 \\\\ \\text{NA} \\\\ %16 \\\\ \\text{NA} \\\\ \\end{array} \\right] \\] We can see that the original missing value, combined with some slightly irregular sampling, created a reconstructed manifold that is mostly missing values! By default, the points which contain missing values will not be added to the library or prediction sets . For example, if we let the library and prediction sets be as big as possible then we will have: \\[ \\mathscr{L} = \\emptyset \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\emptyset \\] \\[ \\mathscr{P} = \\left[ \\begin{array}{cc} 15 & 14 \\\\ \\end{array} \\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = \\left[ \\begin{array}{c} \\text{NA} \\\\ \\end{array} \\right] \\] Here we see that the library set is totally empty! This is because for a point to be in the library (with default options) it must be fully observed and the corresponding \\(y\\) projection must also be observed. Similarly, the prediction set is almost empty because (with default options) it must be fully observed. The allowmissing flag If we set the allowmissing option, then a point is included in the manifold even with some missing values. The only caveats to this rule are: points which are totally missing will always be discarded, we can't have missing targets for points in the library set. The largest possible library and prediction sets with allowmissing in this example would be: \\[ \\mathscr{L} = \\left[\\begin{array}{cc} 14 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\left[ \\begin{array}{c} 15 \\\\ \\end{array} \\right] \\] \\[ \\mathscr{P} = M_x = \\left[\\begin{array}{cc} 11 & \\text{NA} \\\\ %\\text{NA} & 11 \\\\ %\\text{NA} & \\text{NA} \\\\ 12 & \\text{NA} \\\\ \\text{NA} & 12 \\\\ %\\text{NA} & \\text{NA} \\\\ %\\text{NA} & \\text{NA} \\\\ 14 & \\text{NA} \\\\ 15 & 14 \\\\ %\\text{NA} & 15 \\\\ 16 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} \\text{NA} \\\\ %\\text{NA} \\\\ %12 \\\\ \\text{NA} \\\\ \\text{NA} \\\\ %\\text{NA} \\\\ %14 \\\\ 15 \\\\ \\text{NA} \\\\ %16 \\\\ \\text{NA} \\\\ \\end{array} \\right] \\] This discussion is implicitly assuming the algorithm is set to the simplex algorithm. When the S-map algorithm is chosen, then we cannot let missing values into the library set \\(\\mathscr{L}\\) . This may change in a future implementation of the S-map algorithm. The dt flag When we add dt , we tell the package to remove missing observations and to also add the time between the observations into the manifold. So, in this example, instead of the observed time series being: \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 the dt basically acts as if the supplied data were: \\(t\\) \\(x_t\\) \\(\\mathrm{d}t\\) 1.0 11 1.5 2.5 12 2.0 4.5 14 0.5 5.0 15 1.0 6.0 16 NA The resulting manifold and projections are: \\[ M_x = \\left[\\begin{array}{cccc} 12 & 11 & 2.0 & 1.5 \\\\ 14 & 12 & 0.5 & 2.0 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\] The largest possible library and prediction sets with dt in this example would be: \\[ \\mathscr{L} = \\mathscr{P} = M_x = \\left[\\begin{array}{cccc} 12 & 11 & 2.0 & 1.5 \\\\ 14 & 12 & 0.5 & 2.0 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\] Both allowmissing and dt flags If we set both flags, we tell the package to allow missing observations and to also add the time between the observations into the manifold. So our original time series \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 will generate the manifold \\[ M_x = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ 12 & 11 & 0.5 & 1.5 \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ 16 & 15 & \\text{NA} & 1.0 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} 12 \\\\ \\text{NA} \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\text{NA} \\end{array} \\right] \\] and the largest possible library and prediction sets would be \\[ \\mathscr{L} = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\left[ \\begin{array}{c} 12 \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\] \\[ \\mathscr{P} = M_x = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ 12 & 11 & 0.5 & 1.5 \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ 16 & 15 & \\text{NA} & 1.0 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} 12 \\\\ \\text{NA} \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\text{NA} \\end{array} \\right] \\]","title":"Missing data"},{"location":"missing-data/#missing-data","text":"To explain how the package handles missing data given different options, it is easiest to work by example. Let's say we have the following time series and NA represents a missing value: \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 Let's also fix \\(E = 2\\) , \\(\\tau = 1\\) and \\(p = 1\\) for these examples. Here we have one obviously missing value for \\(x\\) at time 3. However, there are some hidden missing values also. By default, the package will assume that your data was measured at a regular time interval and will insert missing values as necessary to create a regular grid. For example, the above time series will be treated as if it were sampled every half time unit. So, when creating the \\(E=2\\) manifold it will \\(t\\) \\(x_t\\) 1.0 11 1.5 NA 2.0 NA 2.5 12 3.0 NA 3.5 NA 4.0 NA 4.5 14 5.0 15 5.5 NA 6.0 16 The manifold of \\(x\\) and it's projections \\(y\\) will have missing values in them: \\[ M_x = \\left[\\begin{array}{cc} 11 & \\text{NA} \\\\ %\\text{NA} & 11 \\\\ %\\text{NA} & \\text{NA} \\\\ 12 & \\text{NA} \\\\ \\text{NA} & 12 \\\\ %\\text{NA} & \\text{NA} \\\\ %\\text{NA} & \\text{NA} \\\\ 14 & \\text{NA} \\\\ 15 & 14 \\\\ %\\text{NA} & 15 \\\\ 16 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} \\text{NA} \\\\ %\\text{NA} \\\\ %12 \\\\ \\text{NA} \\\\ \\text{NA} \\\\ %\\text{NA} \\\\ %14 \\\\ 15 \\\\ \\text{NA} \\\\ %16 \\\\ \\text{NA} \\\\ \\end{array} \\right] \\] We can see that the original missing value, combined with some slightly irregular sampling, created a reconstructed manifold that is mostly missing values! By default, the points which contain missing values will not be added to the library or prediction sets . For example, if we let the library and prediction sets be as big as possible then we will have: \\[ \\mathscr{L} = \\emptyset \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\emptyset \\] \\[ \\mathscr{P} = \\left[ \\begin{array}{cc} 15 & 14 \\\\ \\end{array} \\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = \\left[ \\begin{array}{c} \\text{NA} \\\\ \\end{array} \\right] \\] Here we see that the library set is totally empty! This is because for a point to be in the library (with default options) it must be fully observed and the corresponding \\(y\\) projection must also be observed. Similarly, the prediction set is almost empty because (with default options) it must be fully observed.","title":"Missing data"},{"location":"missing-data/#the-allowmissing-flag","text":"If we set the allowmissing option, then a point is included in the manifold even with some missing values. The only caveats to this rule are: points which are totally missing will always be discarded, we can't have missing targets for points in the library set. The largest possible library and prediction sets with allowmissing in this example would be: \\[ \\mathscr{L} = \\left[\\begin{array}{cc} 14 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\left[ \\begin{array}{c} 15 \\\\ \\end{array} \\right] \\] \\[ \\mathscr{P} = M_x = \\left[\\begin{array}{cc} 11 & \\text{NA} \\\\ %\\text{NA} & 11 \\\\ %\\text{NA} & \\text{NA} \\\\ 12 & \\text{NA} \\\\ \\text{NA} & 12 \\\\ %\\text{NA} & \\text{NA} \\\\ %\\text{NA} & \\text{NA} \\\\ 14 & \\text{NA} \\\\ 15 & 14 \\\\ %\\text{NA} & 15 \\\\ 16 & \\text{NA} \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} \\text{NA} \\\\ %\\text{NA} \\\\ %12 \\\\ \\text{NA} \\\\ \\text{NA} \\\\ %\\text{NA} \\\\ %14 \\\\ 15 \\\\ \\text{NA} \\\\ %16 \\\\ \\text{NA} \\\\ \\end{array} \\right] \\] This discussion is implicitly assuming the algorithm is set to the simplex algorithm. When the S-map algorithm is chosen, then we cannot let missing values into the library set \\(\\mathscr{L}\\) . This may change in a future implementation of the S-map algorithm.","title":"The allowmissing flag"},{"location":"missing-data/#the-dt-flag","text":"When we add dt , we tell the package to remove missing observations and to also add the time between the observations into the manifold. So, in this example, instead of the observed time series being: \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 the dt basically acts as if the supplied data were: \\(t\\) \\(x_t\\) \\(\\mathrm{d}t\\) 1.0 11 1.5 2.5 12 2.0 4.5 14 0.5 5.0 15 1.0 6.0 16 NA The resulting manifold and projections are: \\[ M_x = \\left[\\begin{array}{cccc} 12 & 11 & 2.0 & 1.5 \\\\ 14 & 12 & 0.5 & 2.0 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\] The largest possible library and prediction sets with dt in this example would be: \\[ \\mathscr{L} = \\mathscr{P} = M_x = \\left[\\begin{array}{cccc} 12 & 11 & 2.0 & 1.5 \\\\ 14 & 12 & 0.5 & 2.0 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\]","title":"The dt flag"},{"location":"missing-data/#both-allowmissing-and-dt-flags","text":"If we set both flags, we tell the package to allow missing observations and to also add the time between the observations into the manifold. So our original time series \\(t\\) \\(x_t\\) 1.0 11 2.5 12 3.0 NA 4.5 14 5.0 15 6.0 16 will generate the manifold \\[ M_x = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ 12 & 11 & 0.5 & 1.5 \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ 16 & 15 & \\text{NA} & 1.0 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y = \\left[ \\begin{array}{c} 12 \\\\ \\text{NA} \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\text{NA} \\end{array} \\right] \\] and the largest possible library and prediction sets would be \\[ \\mathscr{L} = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{L}} = \\left[ \\begin{array}{c} 12 \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\end{array} \\right] \\] \\[ \\mathscr{P} = M_x = \\left[\\begin{array}{cccc} 11 & \\text{NA} & 1.5 & \\text{NA} \\\\ 12 & 11 & 0.5 & 1.5 \\\\ \\text{NA} & 12 & 1.5 & 0.5 \\\\ 14 & \\text{NA} & 0.5 & 1.5 \\\\ 15 & 14 & 1.0 & 0.5 \\\\ 16 & 15 & \\text{NA} & 1.0 \\\\ \\end{array}\\right] \\underset{\\small \\text{Matches}}{\\Rightarrow} y^{\\mathscr{P}} = y = \\left[ \\begin{array}{c} 12 \\\\ \\text{NA} \\\\ 14 \\\\ 15 \\\\ 16 \\\\ \\text{NA} \\end{array} \\right] \\]","title":"Both allowmissing and dt flags"},{"location":"time-delayed-embedding/","text":"Creating a time-delayed embedding Imagine that we observe a time series \\(x\\) . Choose the number of observations In tabular form, the data looks like: So each one of \\(x_i\\) is an observation of the \\(x\\) time series. To create a time-delayed embedding based on any of these time series, we first need to choose the size of the embedding \\(E\\) . Choose a value for \\(E\\) The data may be too finely sampled in time. So we select a \\(\\tau\\) which means we only look at every \\(\\tau\\) th observation for each time series. Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with is the manifold: The manifold is a collection of these time-delayed embedding vectors . For short, we just refer to each vector as a point on the manifold. While the manifold notation above is the most accurate (a set of vectors) we will henceforward use the more convenient matrix notation: Note that the manifold has \\(E\\) columns, and the number of rows depends on the number of observations in the \\(x\\) time series. Why does this look backwards? You may wonder why we have each point in the manifold going backwards in time when reading left-to-right. This is simply an unfortunate convention in the EDM literature which we adhere to.","title":"Time-delayed embedding"},{"location":"time-delayed-embedding/#creating-a-time-delayed-embedding","text":"Imagine that we observe a time series \\(x\\) . Choose the number of observations In tabular form, the data looks like: So each one of \\(x_i\\) is an observation of the \\(x\\) time series. To create a time-delayed embedding based on any of these time series, we first need to choose the size of the embedding \\(E\\) . Choose a value for \\(E\\) The data may be too finely sampled in time. So we select a \\(\\tau\\) which means we only look at every \\(\\tau\\) th observation for each time series. Choose a value for \\(\\tau\\) The time-delayed embedding of the \\(x\\) time series with is the manifold: The manifold is a collection of these time-delayed embedding vectors . For short, we just refer to each vector as a point on the manifold. While the manifold notation above is the most accurate (a set of vectors) we will henceforward use the more convenient matrix notation: Note that the manifold has \\(E\\) columns, and the number of rows depends on the number of observations in the \\(x\\) time series. Why does this look backwards? You may wonder why we have each point in the manifold going backwards in time when reading left-to-right. This is simply an unfortunate convention in the EDM literature which we adhere to.","title":"Creating a time-delayed embedding"},{"location":"xmap/","text":"What does edm xmap u v do? Imagine that we use the command: edm xmap u v, oneway This will consider two different time series, here labelled \\(u\\) and \\(v\\) . Choose the number of observations In tabular form, the data looks like: Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The lagged embedding \\(M_u\\) is constructed: The library set is (by default) the first \\(L\\) points of \\(M_u\\) . The library size parameter \\(L\\) is set by the Stata parameter library . Choose a value for \\(L\\) On the other hand, the prediction set will include every point of the \\(u\\) embedding so: The \\(v\\) time series will be the values which we try to predict. Here we are trying to predict \\(p\\) observations ahead, where the default case is actually \\(p = 0\\) . The \\(p = 0\\) case means we are using the \\(u\\) time series to try to predict the contemporaneous value of \\(v\\) . A negative \\(p\\) may be chosen, though this is a bit abnormal. Choose a value for \\(p\\) The prediction procedure is then the same as previous times, though the library and prediction sets all contain values from the \\(u\\) time series whereas the \\(y\\) projection vectors contain (usually contemporaneous) values from the \\(v\\) time series. \\[ \\begin{aligned} \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{\\text{Based on } v} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }_{\\text{Based on } u} \\underset{\\small \\text{Find neighbours in}}{\\Rightarrow} \\underbrace{ \\mathscr{L} }_{\\text{Based on } u} \\\\ &\\,\\,\\,\\,\\underset{\\small \\text{Matches}}{\\Rightarrow} \\underbrace{ \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} }_{\\text{Based on } v} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\underbrace{ \\hat{y}_i^{\\mathscr{P}} }_{\\text{Based on } v} \\end{aligned} \\]","title":"The xmap subcommand"},{"location":"xmap/#what-does-edm-xmap-u-v-do","text":"Imagine that we use the command: edm xmap u v, oneway This will consider two different time series, here labelled \\(u\\) and \\(v\\) . Choose the number of observations In tabular form, the data looks like: Choose a value for \\(E\\) Choose a value for \\(\\tau\\) The lagged embedding \\(M_u\\) is constructed: The library set is (by default) the first \\(L\\) points of \\(M_u\\) . The library size parameter \\(L\\) is set by the Stata parameter library . Choose a value for \\(L\\) On the other hand, the prediction set will include every point of the \\(u\\) embedding so: The \\(v\\) time series will be the values which we try to predict. Here we are trying to predict \\(p\\) observations ahead, where the default case is actually \\(p = 0\\) . The \\(p = 0\\) case means we are using the \\(u\\) time series to try to predict the contemporaneous value of \\(v\\) . A negative \\(p\\) may be chosen, though this is a bit abnormal. Choose a value for \\(p\\) The prediction procedure is then the same as previous times, though the library and prediction sets all contain values from the \\(u\\) time series whereas the \\(y\\) projection vectors contain (usually contemporaneous) values from the \\(v\\) time series. \\[ \\begin{aligned} \\underbrace{ \\text{For target }y_i^{\\mathscr{P}} }_{\\text{Based on } v} & \\underset{\\small \\text{Get predictee}}{\\Rightarrow} \\underbrace{ \\mathscr{P}_{i} }_{\\text{Based on } u} \\underset{\\small \\text{Find neighbours in}}{\\Rightarrow} \\underbrace{ \\mathscr{L} }_{\\text{Based on } u} \\\\ &\\,\\,\\,\\,\\underset{\\small \\text{Matches}}{\\Rightarrow} \\underbrace{ \\{ y_j^{\\,\\mathscr{L}} \\}_{j \\in \\mathcal{NN}_k(i)} }_{\\text{Based on } v} \\underset{\\small \\text{Make prediction}}{\\Rightarrow} \\underbrace{ \\hat{y}_i^{\\mathscr{P}} }_{\\text{Based on } v} \\end{aligned} \\]","title":"What does edm xmap u v do?"},{"location":"examples/chicago/","text":"Chicago crime/temperature example To demonstrate the usefulness of EDM in estimating the impact of causal variables, we use a real-world dataset that reflects daily temperature and crime levels in Chicago, which we make available in the chicago.dta file. The data To demonstrate the usefulness of EDM in estimating the impact of causal variables, we use a real-world dataset that reflects daily temperature and crime levels in Chicago, which we make available in the chicago.dta file. First, we load the time series from the chicago.dta file: . use chicago, clear Plotting the data gives: . scatter crime temp A linear correlation of the dataset... . corr crime temp (obs= 4 , 371 ) | crime temp -------------+------------------ crime | 1.0000 temp | 0.4620 1.0000 shows a mild correlation, however the causal structure (if any) and its direction is not shown. Find the optimal embedding dimension Now we use edm explore to find the optimal embedding dimension of the \\(\\texttt{Temperature}\\) time series. We check the values of \\(E = 2, \\dots 20\\) . The crossfold(5) option means that, for each \\(E\\) value we run 5 sets of predictions, and for each set we take four fifths of the data for training and predict the remaining one fifth. . edm explore temp, e ( 2 / 20 ) crossfold( 5 ) 5 - fold cross - validation progress ( 5 in total ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with temp and its lag values ---------------------------------------------------------------------- --------- rho --------- --------- MAE --------- Actual E theta Mean Std. Dev. Mean Std. Dev. ---------------------------------------------------------------------- 2 1 . 89818 . 010487 7.1032 . 20784 3 1 . 90439 . 0080387 6.9064 . 27909 4 1 . 91028 . 0095908 6.6776 . 29265 5 1 . 91548 . 010502 6.4794 . 37096 6 1 . 91879 . 0078932 6.4002 . 30358 7 1 . 91934 . 0071129 6.3696 . 2972 8 1 . 91798 . 0070104 6.4607 . 25364 9 1 . 91722 . 0071189 6.5027 . 27352 10 1 . 91762 . 0073315 6.5053 . 28581 11 1 . 91561 . 0077983 6.5573 . 31192 12 1 . 91638 . 007973 6.5689 . 32428 13 1 . 91524 . 0083123 6.6081 . 31755 14 1 . 91485 . 0086085 6.6257 . 35135 15 1 . 9133 . 0083516 6.6861 . 32796 16 1 . 91289 . 0092333 6.7024 . 35415 17 1 . 91183 . 0094544 6.7338 . 35987 18 1 . 91029 . 0089739 6.7972 . 35502 19 1 . 90853 . 008125 6.8699 . 33429 20 1 . 90766 . 0091983 6.9047 . 31412 ---------------------------------------------------------------------- Note: Results from 5 runs Note: Number of neighbours (k) is set to between 3 and 21 Note: 5 - fold cross validation results reported From the rho column we can see that the prediction accuracy is maximised when \\(E = 7\\) , so we take this as our estimate of the embedding dimension. Convergent cross-mapping The edm xmap command will run the cross-mapping task, which allows us to ascertain the causal links between the crime and temperature time series. . qui edm xmap temp crime, e ( 7 ) library( 10 ( 5 ) 200 210 ( 10 ) 1000 1020 ( 20 ) 2000 2050 ( > 50 ) 4350 4365 ) rep( 4 ) Note This selects a lot of library points, and replicates the analysis some times, so this command may take a minute or two to finish. Choosing a machine with more CPU cores or faster cores will help significantly. Plotting the results gives: . mat cyx = e (xmap_2) . mat cxy = e (xmap_1) . svmat cyx, names(chicago_yx) . svmat cxy, names(chicago_xy) . label variable chicago_xy3 \"Crime|M(Temperature)\" . label variable chicago_yx3 \"Temperature|M(Crime)\" . twoway (scatter chicago_xy3 chicago_xy2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (scatter chicago_yx3 chicago_yx2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (lpoly chicago_xy3 chicago_xy2)(lpoly chicago_yx3 chicago_yx2), xtitle(L) > ytitle( \"{it:{&rho}}\" ) legend(col( 1 )) . drop chicago_xy * chicago_yx * In this plot, we can see that one direction shows a significant increase in accuracy as \\(L\\) increases, whereas the other direction is pretty flat. The direction which increases the most is the \\(\\texttt{Temperature} \\mid M(\\texttt{Crime})\\) direction. This notation means we used \\(\\texttt{Crime}\\) to predict \\(\\texttt{Temperature}\\) , and due to the backward nature of EDM means it refers to the causal link \\(\\texttt{Temperature} \\to M(\\texttt{Crime})\\) . Therefore, we'd conclude that there is a causal link from temperature to crime, though no link in the reverse direction (which would be implausible). Inspecting the S-map coefficients If we run xmap with the savesmap(beta) option, we can store the fitted S-map coefficients into variable which start with the prefix beta . . edm xmap temp crime, e ( 7 ) alg(smap) k( - 1 ) savesmap(beta) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Convergent Cross - mapping result for variables temp and crime -------------------------------------------------------------------------- Mapping Library size rho MAE -------------------------------------------------------------------------- crime ~ crime|M(temp) 4365 . 46784 136.93 temp ~ temp|M(crime) 4365 . 54886 14.661 -------------------------------------------------------------------------- Note: Number of neighbours (k) is set to 4364 Note: The embedding dimension E is 7 For example, the coefficient variables that are created are: . ds beta * , detail Variable Storage Display Value name type format label Variable label ------------------------------------------------------------------------------- beta1_b0_rep1 double %10.0g constant in temp predicting crime S - map equation (rep 1 ) beta1_b1_rep1 double %10.0g temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b2_rep1 double %10.0g l1 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b3_rep1 double %10.0g l2 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b4_rep1 double %10.0g l3 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b5_rep1 double %10.0g l4 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b6_rep1 double %10.0g l5 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b7_rep1 double %10.0g l6 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta2_b0_rep1 double %10.0g constant in crime predicting temp S - map equation (rep 1 ) beta2_b1_rep1 double %10.0g crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b2_rep1 double %10.0g l1 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b3_rep1 double %10.0g l2 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b4_rep1 double %10.0g l3 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b5_rep1 double %10.0g l4 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b6_rep1 double %10.0g l5 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b7_rep1 double %10.0g l6 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) Plotting them allows us to see the contemporaneous effect of temperature on crime. . twoway (kdensity beta1_b1_rep1), xtitle( \"Contemporaneous effect of temperatur > e on crime \") ytitle(\" Density \") . twoway (scatter beta1_b1_rep1 temp, xtitle( \"Temperature (Fahrenheit)\" ) ytitle > ( \"Contemporaneous effect of temperature on crime\" ) msize(small))(lpoly beta1_ > b1_rep1 temp), legend(on order ( 1 \"Local coefficient\" 2 \"Local polynomial smo > othing \"))","title":"Chicago crime/temperature"},{"location":"examples/chicago/#chicago-crimetemperature-example","text":"To demonstrate the usefulness of EDM in estimating the impact of causal variables, we use a real-world dataset that reflects daily temperature and crime levels in Chicago, which we make available in the chicago.dta file.","title":"Chicago crime/temperature example"},{"location":"examples/chicago/#the-data","text":"To demonstrate the usefulness of EDM in estimating the impact of causal variables, we use a real-world dataset that reflects daily temperature and crime levels in Chicago, which we make available in the chicago.dta file. First, we load the time series from the chicago.dta file: . use chicago, clear Plotting the data gives: . scatter crime temp A linear correlation of the dataset... . corr crime temp (obs= 4 , 371 ) | crime temp -------------+------------------ crime | 1.0000 temp | 0.4620 1.0000 shows a mild correlation, however the causal structure (if any) and its direction is not shown.","title":"The data"},{"location":"examples/chicago/#find-the-optimal-embedding-dimension","text":"Now we use edm explore to find the optimal embedding dimension of the \\(\\texttt{Temperature}\\) time series. We check the values of \\(E = 2, \\dots 20\\) . The crossfold(5) option means that, for each \\(E\\) value we run 5 sets of predictions, and for each set we take four fifths of the data for training and predict the remaining one fifth. . edm explore temp, e ( 2 / 20 ) crossfold( 5 ) 5 - fold cross - validation progress ( 5 in total ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with temp and its lag values ---------------------------------------------------------------------- --------- rho --------- --------- MAE --------- Actual E theta Mean Std. Dev. Mean Std. Dev. ---------------------------------------------------------------------- 2 1 . 89818 . 010487 7.1032 . 20784 3 1 . 90439 . 0080387 6.9064 . 27909 4 1 . 91028 . 0095908 6.6776 . 29265 5 1 . 91548 . 010502 6.4794 . 37096 6 1 . 91879 . 0078932 6.4002 . 30358 7 1 . 91934 . 0071129 6.3696 . 2972 8 1 . 91798 . 0070104 6.4607 . 25364 9 1 . 91722 . 0071189 6.5027 . 27352 10 1 . 91762 . 0073315 6.5053 . 28581 11 1 . 91561 . 0077983 6.5573 . 31192 12 1 . 91638 . 007973 6.5689 . 32428 13 1 . 91524 . 0083123 6.6081 . 31755 14 1 . 91485 . 0086085 6.6257 . 35135 15 1 . 9133 . 0083516 6.6861 . 32796 16 1 . 91289 . 0092333 6.7024 . 35415 17 1 . 91183 . 0094544 6.7338 . 35987 18 1 . 91029 . 0089739 6.7972 . 35502 19 1 . 90853 . 008125 6.8699 . 33429 20 1 . 90766 . 0091983 6.9047 . 31412 ---------------------------------------------------------------------- Note: Results from 5 runs Note: Number of neighbours (k) is set to between 3 and 21 Note: 5 - fold cross validation results reported From the rho column we can see that the prediction accuracy is maximised when \\(E = 7\\) , so we take this as our estimate of the embedding dimension.","title":"Find the optimal embedding dimension"},{"location":"examples/chicago/#convergent-cross-mapping","text":"The edm xmap command will run the cross-mapping task, which allows us to ascertain the causal links between the crime and temperature time series. . qui edm xmap temp crime, e ( 7 ) library( 10 ( 5 ) 200 210 ( 10 ) 1000 1020 ( 20 ) 2000 2050 ( > 50 ) 4350 4365 ) rep( 4 ) Note This selects a lot of library points, and replicates the analysis some times, so this command may take a minute or two to finish. Choosing a machine with more CPU cores or faster cores will help significantly. Plotting the results gives: . mat cyx = e (xmap_2) . mat cxy = e (xmap_1) . svmat cyx, names(chicago_yx) . svmat cxy, names(chicago_xy) . label variable chicago_xy3 \"Crime|M(Temperature)\" . label variable chicago_yx3 \"Temperature|M(Crime)\" . twoway (scatter chicago_xy3 chicago_xy2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (scatter chicago_yx3 chicago_yx2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (lpoly chicago_xy3 chicago_xy2)(lpoly chicago_yx3 chicago_yx2), xtitle(L) > ytitle( \"{it:{&rho}}\" ) legend(col( 1 )) . drop chicago_xy * chicago_yx * In this plot, we can see that one direction shows a significant increase in accuracy as \\(L\\) increases, whereas the other direction is pretty flat. The direction which increases the most is the \\(\\texttt{Temperature} \\mid M(\\texttt{Crime})\\) direction. This notation means we used \\(\\texttt{Crime}\\) to predict \\(\\texttt{Temperature}\\) , and due to the backward nature of EDM means it refers to the causal link \\(\\texttt{Temperature} \\to M(\\texttt{Crime})\\) . Therefore, we'd conclude that there is a causal link from temperature to crime, though no link in the reverse direction (which would be implausible).","title":"Convergent cross-mapping"},{"location":"examples/chicago/#inspecting-the-s-map-coefficients","text":"If we run xmap with the savesmap(beta) option, we can store the fitted S-map coefficients into variable which start with the prefix beta . . edm xmap temp crime, e ( 7 ) alg(smap) k( - 1 ) savesmap(beta) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Convergent Cross - mapping result for variables temp and crime -------------------------------------------------------------------------- Mapping Library size rho MAE -------------------------------------------------------------------------- crime ~ crime|M(temp) 4365 . 46784 136.93 temp ~ temp|M(crime) 4365 . 54886 14.661 -------------------------------------------------------------------------- Note: Number of neighbours (k) is set to 4364 Note: The embedding dimension E is 7 For example, the coefficient variables that are created are: . ds beta * , detail Variable Storage Display Value name type format label Variable label ------------------------------------------------------------------------------- beta1_b0_rep1 double %10.0g constant in temp predicting crime S - map equation (rep 1 ) beta1_b1_rep1 double %10.0g temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b2_rep1 double %10.0g l1 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b3_rep1 double %10.0g l2 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b4_rep1 double %10.0g l3 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b5_rep1 double %10.0g l4 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b6_rep1 double %10.0g l5 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta1_b7_rep1 double %10.0g l6 . temp predicting crime or crime|M(temp) S - map coefficient (rep 1 ) beta2_b0_rep1 double %10.0g constant in crime predicting temp S - map equation (rep 1 ) beta2_b1_rep1 double %10.0g crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b2_rep1 double %10.0g l1 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b3_rep1 double %10.0g l2 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b4_rep1 double %10.0g l3 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b5_rep1 double %10.0g l4 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b6_rep1 double %10.0g l5 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) beta2_b7_rep1 double %10.0g l6 . crime predicting temp or temp|M(crime) S - map coefficient (rep 1 ) Plotting them allows us to see the contemporaneous effect of temperature on crime. . twoway (kdensity beta1_b1_rep1), xtitle( \"Contemporaneous effect of temperatur > e on crime \") ytitle(\" Density \") . twoway (scatter beta1_b1_rep1 temp, xtitle( \"Temperature (Fahrenheit)\" ) ytitle > ( \"Contemporaneous effect of temperature on crime\" ) msize(small))(lpoly beta1_ > b1_rep1 temp), legend(on order ( 1 \"Local coefficient\" 2 \"Local polynomial smo > othing \"))","title":"Inspecting the S-map coefficients"},{"location":"examples/logistic-map/","text":"Setting up the synthetic data First, we tell Stata that we're working with time series using tsset . . set obs 500 Number of observations (_N) was 0 , now 500 . . . gen t = _n . tsset t Time variable: t, 1 to 500 Delta: 1 unit This is important, as the edm relies on the user to specify which variable corresponds to 'time'. Next, we can generate the synthetic data which we will use in the causal analysis. . gen x = 0.2 if _n == 1 ( 499 missing values generated) . gen y = 0.3 if _n == 1 ( 499 missing values generated) . gen z = 0.1 if _n == 1 ( 499 missing values generated) . . local r_x 3.79 . local r_y 3.79 . local r_z 3.77 . local beta_xy = 0.0 . local beta_yx= 0.2 . local beta_zy = 0.5 . local tau = 1 . drawnorm u1 u2 . . forvalues i= 2 / `=_N' { 2 . qui replace x=l . x * ( `r_x' * ( 1 - l . x) - `beta_xy' * l . y) in `i' 3 . qui replace y=l . y * ( `r_y' * ( 1 - l . y) - `beta_yx' * l `tau' .x) in `i' 4 . qui replace z=l . z * ( `r_z' * ( 1 - l . z) - `beta_zy' * l `tau' .y) in `i' 5 . } . keep in 300 / 450 ( 349 observations deleted) Now we have two time series, \\(x\\) and \\(y\\) . For example, the first ten joint observations look like: . list x y if _n <= 10 +---------------------+ | x y | | --------------------- | 1 . | . 5353563 . 3485506 | 2 . | . 9427623 . 8232493 | 3 . | . 2045144 . 3962567 | 4 . | . 6165885 . 8905014 | 5 . | . 895983 . 2597431 | | --------------------- | 6 . | . 3532184 . 6821833 | 7 . | . 8658451 . 773515 | 8 . | . 4402364 . 5300195 | 9 . | . 9339633 . 8974178 | 10 . | . 2337515 . 181273 | +---------------------+ Plotting the two time series together looks like: Find the optimal embedding dimension Now we use edm explore to find the optimal embedding dimension of the \\(y\\) time series. We check the values of \\(E = 2, \\dots 10\\) , and use rep(50) to take 50 random subsets of the data to use for training (leaving the other random half for prediction). . edm explore y, e ( 2 / 10 ) rep( 50 ) Replication progress ( 50 in total ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with y and its lag values ---------------------------------------------------------------------- --------- rho --------- --------- MAE --------- Actual E theta Mean Std. Dev. Mean Std. Dev. ---------------------------------------------------------------------- 2 1 . 98104 . 0056417 . 03127 . 0044271 3 1 . 96996 . 012497 . 0393 . 0054778 4 1 . 95131 . 016597 . 049013 . 0060371 5 1 . 92509 . 02697 . 059667 . 0082334 6 1 . 89149 . 039457 . 07092 . 0097508 7 1 . 83573 . 04969 . 085179 . 010616 8 1 . 78633 . 058107 . 098735 . 011921 9 1 . 74695 . 066014 . 10839 . 012128 10 1 . 71795 . 071137 . 11598 . 011756 ---------------------------------------------------------------------- Note: Results from 50 runs Note: Number of neighbours (k) is set to between 3 and 11 Note: Random 50 / 50 split for training and validation data . mat r= e (explore_result) . svmat r, names(col) number of observations will be reset to 450 Press any key to continue , or Break to abort Number of observations (_N) was 151 , now 450 . . twoway (scatter c3 c1)(lpoly c3 c1),xtitle( \"E\" ) ytitle( \"{it:{&rho}}\" ) legend( > order ( 1 \"{it:{&rho}}\" 2 \"local polynomial smoothing\" ) col( 1 ) position( 8 ) ring > ( 0 )) . drop c * From the rho column we can see the prediction accuracy decreasing as \\(E\\) increases, so \\(E=2\\) is our best choice for the embedding dimension. Plotting the same results: . mat r= e (explore_result) . svmat r, names(col) . twoway (scatter c3 c1)(lpoly c3 c1),xtitle( \"E\" ) ytitle( \"{it:{&rho}}\" ) legend( > order ( 1 \"{it:{&rho}}\" 2 \"local polynomial smoothing\" ) col( 1 ) position( 8 ) ring > ( 0 )) . drop c * Assess the level of non-linearity in the time series Another use of edm explore is to check if the observed time series exhibits high levels of non-linearity. Here, we use the S-map algorithm and vary \\(\\theta = 0, \\dots, 5\\) , using all of the training set as neighbours (i.e. k(-1) ). . edm explore y, e ( 2 ) algorithm(smap) theta( 0 ( 1 ) 5 ) k( - 1 ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with y and its lag values -------------------------------------------------------------------- Actual E theta rho MAE -------------------------------------------------------------------- 2 0 . 7276 . 12482 2 1 . 9594 . 057205 2 2 . 98107 . 035173 2 3 . 98825 . 027744 2 4 . 99162 . 023781 2 5 . 993 . 021781 -------------------------------------------------------------------- Note: Number of neighbours (k) is set to 75 Note: 50 / 50 split for training and validation data Showing the same rho / \\(\\rho\\) prediction accuracies as a plot: . mat r = e (explore_result) . svmat r, names(col) number of observations will be reset to 501 Press any key to continue , or Break to abort Number of observations (_N) was 450 , now 501 . . twoway (line c3 c2) , legend(order( 1 \"{it:{&rho}}\" ) position( 5 ) ring( 0 )) xtit > le( \"{it:{&theta}}\" ) ytitle( \"{it:{&rho}}\" ) title( \"{it:{&rho}}-{it:{&theta}} of > variable y \") . drop c * As the accuracy climbs as larger for \\(\\theta > 0\\) compared to \\(\\theta = 0\\) , we deduce that the time series is likely the output of a non-linear system. This is important, as the theory underlying EDM is specific to non-linear systems. Convergent cross-mapping Finally, now we are satisfied that the time series are non-linear and we have selected \\(E=2\\) as the embedding dimension, we can run convergent cross-mappping . If the prediction accuracy increases as the library size \\(L\\) increases, then we can say this is evidence of a causal link in that direction. . qui edm xmap x y, e ( 2 ) rep( 10 ) library( 5 / 150 ) Using these predictions, we can plot the accuracy against the library size: . mat c1 = e (xmap_1) . mat c2 = e (xmap_2) . svmat c1, names(xy) number of observations will be reset to 1460 Press any key to continue , or Break to abort Number of observations (_N) was 501 , now 1 , 460 . . svmat c2, names(yx) . label variable xy3 \"y|M(x)\" . label variable yx3 \"x|M(y)\" . twoway (scatter xy3 xy2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (scatter yx3 yx2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (lpoly xy3 xy2)(lpoly yx3 yx2), xtitle(L) ytitle( \"{it:{&rho}}\" ) legend(co > l ( 2 )) As both plots of the accuracy are significantly increasing as \\(L\\) increases, then we can say there is evidence of both \\(x \\to y\\) and \\(y \\to x\\) causal links. The direction which increases the most is the \\(x \\mid M(y)\\) direction. This notation means we used \\(y\\) to predict \\(x\\) , and due to the backward nature of EDM means it refers to the causal link \\(x \\to y\\) .","title":"Logistic map"},{"location":"examples/logistic-map/#setting-up-the-synthetic-data","text":"First, we tell Stata that we're working with time series using tsset . . set obs 500 Number of observations (_N) was 0 , now 500 . . . gen t = _n . tsset t Time variable: t, 1 to 500 Delta: 1 unit This is important, as the edm relies on the user to specify which variable corresponds to 'time'. Next, we can generate the synthetic data which we will use in the causal analysis. . gen x = 0.2 if _n == 1 ( 499 missing values generated) . gen y = 0.3 if _n == 1 ( 499 missing values generated) . gen z = 0.1 if _n == 1 ( 499 missing values generated) . . local r_x 3.79 . local r_y 3.79 . local r_z 3.77 . local beta_xy = 0.0 . local beta_yx= 0.2 . local beta_zy = 0.5 . local tau = 1 . drawnorm u1 u2 . . forvalues i= 2 / `=_N' { 2 . qui replace x=l . x * ( `r_x' * ( 1 - l . x) - `beta_xy' * l . y) in `i' 3 . qui replace y=l . y * ( `r_y' * ( 1 - l . y) - `beta_yx' * l `tau' .x) in `i' 4 . qui replace z=l . z * ( `r_z' * ( 1 - l . z) - `beta_zy' * l `tau' .y) in `i' 5 . } . keep in 300 / 450 ( 349 observations deleted) Now we have two time series, \\(x\\) and \\(y\\) . For example, the first ten joint observations look like: . list x y if _n <= 10 +---------------------+ | x y | | --------------------- | 1 . | . 5353563 . 3485506 | 2 . | . 9427623 . 8232493 | 3 . | . 2045144 . 3962567 | 4 . | . 6165885 . 8905014 | 5 . | . 895983 . 2597431 | | --------------------- | 6 . | . 3532184 . 6821833 | 7 . | . 8658451 . 773515 | 8 . | . 4402364 . 5300195 | 9 . | . 9339633 . 8974178 | 10 . | . 2337515 . 181273 | +---------------------+ Plotting the two time series together looks like:","title":"Setting up the synthetic data"},{"location":"examples/logistic-map/#find-the-optimal-embedding-dimension","text":"Now we use edm explore to find the optimal embedding dimension of the \\(y\\) time series. We check the values of \\(E = 2, \\dots 10\\) , and use rep(50) to take 50 random subsets of the data to use for training (leaving the other random half for prediction). . edm explore y, e ( 2 / 10 ) rep( 50 ) Replication progress ( 50 in total ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with y and its lag values ---------------------------------------------------------------------- --------- rho --------- --------- MAE --------- Actual E theta Mean Std. Dev. Mean Std. Dev. ---------------------------------------------------------------------- 2 1 . 98104 . 0056417 . 03127 . 0044271 3 1 . 96996 . 012497 . 0393 . 0054778 4 1 . 95131 . 016597 . 049013 . 0060371 5 1 . 92509 . 02697 . 059667 . 0082334 6 1 . 89149 . 039457 . 07092 . 0097508 7 1 . 83573 . 04969 . 085179 . 010616 8 1 . 78633 . 058107 . 098735 . 011921 9 1 . 74695 . 066014 . 10839 . 012128 10 1 . 71795 . 071137 . 11598 . 011756 ---------------------------------------------------------------------- Note: Results from 50 runs Note: Number of neighbours (k) is set to between 3 and 11 Note: Random 50 / 50 split for training and validation data . mat r= e (explore_result) . svmat r, names(col) number of observations will be reset to 450 Press any key to continue , or Break to abort Number of observations (_N) was 151 , now 450 . . twoway (scatter c3 c1)(lpoly c3 c1),xtitle( \"E\" ) ytitle( \"{it:{&rho}}\" ) legend( > order ( 1 \"{it:{&rho}}\" 2 \"local polynomial smoothing\" ) col( 1 ) position( 8 ) ring > ( 0 )) . drop c * From the rho column we can see the prediction accuracy decreasing as \\(E\\) increases, so \\(E=2\\) is our best choice for the embedding dimension. Plotting the same results: . mat r= e (explore_result) . svmat r, names(col) . twoway (scatter c3 c1)(lpoly c3 c1),xtitle( \"E\" ) ytitle( \"{it:{&rho}}\" ) legend( > order ( 1 \"{it:{&rho}}\" 2 \"local polynomial smoothing\" ) col( 1 ) position( 8 ) ring > ( 0 )) . drop c *","title":"Find the optimal embedding dimension"},{"location":"examples/logistic-map/#assess-the-level-of-non-linearity-in-the-time-series","text":"Another use of edm explore is to check if the observed time series exhibits high levels of non-linearity. Here, we use the S-map algorithm and vary \\(\\theta = 0, \\dots, 5\\) , using all of the training set as neighbours (i.e. k(-1) ). . edm explore y, e ( 2 ) algorithm(smap) theta( 0 ( 1 ) 5 ) k( - 1 ) Percent complete: 0 ... 10 ... 20 ... 30 ... 40 ... 50 ... 60 ... 70 ... 80 ... 90 ... Empirical Dynamic Modelling Univariate mapping with y and its lag values -------------------------------------------------------------------- Actual E theta rho MAE -------------------------------------------------------------------- 2 0 . 7276 . 12482 2 1 . 9594 . 057205 2 2 . 98107 . 035173 2 3 . 98825 . 027744 2 4 . 99162 . 023781 2 5 . 993 . 021781 -------------------------------------------------------------------- Note: Number of neighbours (k) is set to 75 Note: 50 / 50 split for training and validation data Showing the same rho / \\(\\rho\\) prediction accuracies as a plot: . mat r = e (explore_result) . svmat r, names(col) number of observations will be reset to 501 Press any key to continue , or Break to abort Number of observations (_N) was 450 , now 501 . . twoway (line c3 c2) , legend(order( 1 \"{it:{&rho}}\" ) position( 5 ) ring( 0 )) xtit > le( \"{it:{&theta}}\" ) ytitle( \"{it:{&rho}}\" ) title( \"{it:{&rho}}-{it:{&theta}} of > variable y \") . drop c * As the accuracy climbs as larger for \\(\\theta > 0\\) compared to \\(\\theta = 0\\) , we deduce that the time series is likely the output of a non-linear system. This is important, as the theory underlying EDM is specific to non-linear systems.","title":"Assess the level of non-linearity in the time series"},{"location":"examples/logistic-map/#convergent-cross-mapping","text":"Finally, now we are satisfied that the time series are non-linear and we have selected \\(E=2\\) as the embedding dimension, we can run convergent cross-mappping . If the prediction accuracy increases as the library size \\(L\\) increases, then we can say this is evidence of a causal link in that direction. . qui edm xmap x y, e ( 2 ) rep( 10 ) library( 5 / 150 ) Using these predictions, we can plot the accuracy against the library size: . mat c1 = e (xmap_1) . mat c2 = e (xmap_2) . svmat c1, names(xy) number of observations will be reset to 1460 Press any key to continue , or Break to abort Number of observations (_N) was 501 , now 1 , 460 . . svmat c2, names(yx) . label variable xy3 \"y|M(x)\" . label variable yx3 \"x|M(y)\" . twoway (scatter xy3 xy2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (scatter yx3 yx2, mfcolor(% 30 ) mlcolor(% 30 )) /// > (lpoly xy3 xy2)(lpoly yx3 yx2), xtitle(L) ytitle( \"{it:{&rho}}\" ) legend(co > l ( 2 )) As both plots of the accuracy are significantly increasing as \\(L\\) increases, then we can say there is evidence of both \\(x \\to y\\) and \\(y \\to x\\) causal links. The direction which increases the most is the \\(x \\mid M(y)\\) direction. This notation means we used \\(y\\) to predict \\(x\\) , and due to the backward nature of EDM means it refers to the causal link \\(x \\to y\\) .","title":"Convergent cross-mapping"}]}